{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:32, 5.23MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/cifar/cifar-10-python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHF9JREFUeJzt3UmPZOl1HuAvxsyMrKzKqsqau6rYA5vNbropkjJJmYIsUIBXWtn+BV7YO/8Yr73wymtDNAwIggwSMEmBNMeW2Wz2VOzumquyco6M2QttzI2Bc5gChYPn2Z88Ed+9cd+8q7ezWq0aAFBT9w/9AQCAfzyCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh/T/0B/jH8l/+w79fZebGx9PwTK+f+3+pc/tGeGZvtJHa9faFYWruk1/+LDzznR/+PLVrbzILz/R6ybPvdFJzg7X18MylKzupXec34t/t83eupHb9+be+Hp6Zz+LXq7XWnu0fpeYGWxfDM+9+8NvUrr/97g/jQ8nnwNogN3dhMAjPDPuL1K5p4lrPZ7nfWFstU2NrvbXwzMkq/rxvrbUXp/F46eZ+Lu073/+75EH+P7t/3z8AAPzTJegBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGFl2+te3P84NddfxJuTBv1UUV67v5qEZ94f5yqQ3v7iK6m55TT+Ga/t5NraNlLfLXf22fa6k0n8PPZ3X6R2HXXiTWOT03Fq15e/+o3wzOzkNLXr2fPceVxbjzc3LqcHqV0ba/H7atlyrWtXt86l5r70ymvhmadP7qd2jceH4Zmjo1xLYevGW/laa22tPw/P3Lx+IbVrNrwanvngV/dSu86CN3oAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZUpuPT9dScyfj/fDMsJMr92iLeKFCtzNMrXr228epuZ88+Cw88+snudKS1SReSpEtp1lfX0/NzebxopnWzf0/vb4Rv4f3xrlilR+983545sblXCHIZJ67ZpkCo7XkE24wSHzG3NG3L7z6amruc3fuhme2t0apXY8e3gvPLGe55+K5izdSc4tBvPRotJYr3rm5Ey8i+rSXO/uz4I0eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtdeNeriFrtxtvJ+ssJqldl/vx4z93/mJq1+lxvJWvtdb2DuPf7eB0ltq1Spz9YpFok2ut9ZKfsZ/533gWb11rrbXjafzsz61yu370i1+GZ15/7bXUrjdevZOa6w/j7V+f+1yuGe54OQjPPH74NLXr4HCcmmvrm+GRP/6zt1Orfv7j74VnxvN4G2VrrR3Oci1vz4/jz8ZL41zD3q3eYXjm9Cjb2vj780YPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAorW2qz1tlNzd0YxYsYtlu8AKO11i5d3AjPfLyKlym01trmxjI1t9aJl6SMOrnbara5Fp+Z58ppTie5IqJF4n/jjVGupGO4Fr+vrt++kdp186Xb4ZlnR7lCkEcHuRKXb3zj6+GZ3cePUrv+9b/5Vnjmf/z3v07t+uEP/i41d+dLXw3PfPvtr6V2fXj/o/DMx9//cWrX/nQrNXc0jz/jvvjP42fYWmvj2YvwzM7OemrXWfBGDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZ9rrhZu6rvbJ1NTzz8iq368Iw0Wa0/1lq12g73gzXWmvHw5PwzHKwSO364z+KN0lduxq/Xq219tEHH6TmPv3kfnim28u1G67m8Xa49W7u7P/kG/Gzfxq/NVprrf3oe99Nzb333p3wzGKc/JCbF8Mje8e5RsSjWe5964OHz8Mzx8teatfxPP4Zn+zlzmOyfi419/m7r4Rntq/dTO16+jx+9t/+9lupXWfBGz0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0BhZdvrjqa5xrALvc3wzOzZi9SuT/fiTWh/+uU3UrvG0+PU3K1lfGZ9tErt+uZ2/OzfvLKT2nWyzH3GZ2vxFsCT/dz9sZjGZ/rTw9Suu598HJ7Z2Jundl26sp2am/39z8Iz2ebAH/7q3fDMew8epHadznMtb/c/iTdZPnn+NLXr61/5Znjm7vbt1K7/9F//W2puOn4UnvnJj5+ldj1+/GF45qt/kXt2nwVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgsLKlNld666m5W60Xnjl/fiu16+cv4qUULyb7qV13r99Izf3bJy+HZwYHuQKdy+/Hz2Ptw4epXYvlLDX3uU58ZrBIDLXWuv34Pbzo5EpcJj/6aXjmQrKMZbkTLy9qrbXFPNGwdLBI7TrfOxeemRzn7vtL8UdOa6210Wocnjl49NvUrltffD08s7WZewZ//dVbqbkn+/EWqEdHJ6ldJye74ZmP3n8/tesseKMHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAorGx73Rtbo9Tc5vNn4ZleN9Gq1Vp7/aWXwjOHj5+mdrVVrkHtVmcVnhkNc7t6iUaozjL++VprLd5z9Q8m3cT/xsO11K7BKv7d+pmGt9baoBtv85tt5WrXVie51rv5JH4ei5a7F69143fItzdyrXzTzjA1t7h5LTyzfu9eatdJ5iMmWz3feuO11NyNk/g1uzGbp3a9/urN8MxrO/FGxLPijR4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFFa21Gb3wUepuck8XoIx7uWKRE4uxEsONk7i5SOttXb67oepuUVvEZ6Zb+Zuq24vXkqxlixx6bT11Nw8UQ60WOY+42owiM+kNuXm+ldfSe3a2su9X5wmLtn07sXUrovzo/DM5mmuKmm+lytWOXqyH545efD91K6H//sX4Znzb72e2vX8Ua64azq6FJ6Zj1Or2snzF+GZg0G2Suv3540eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtdc+P9lJznx6fhmfmy1z71LBzPTwzuriT2vV8fJiau95bC89snOb+f1wcxJv5JtNcm1/byZ3j5uuvhWdOE01orbV29OwgPLO2jLfrtdZabzIJz0ye5u6ptpZrlOtsx9se+51cn9/yIP4c2Hgr1+bXhvHv1Vproyfx6rXj+/dTu/Z+/UF4ZvnJ49SurUtbqbnd7XhL5PNHud/mwyefhWdeHt5I7ToL3ugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKK9te9+I03j7VWmuPTuJtRrOD49SunWtXwjOr21dTu9Yu5hqh1g7izXz9B09Tu6ZHJ+GZoxZvrGqttcW5jdTc4O6d8Ey/s0jt2tyOn8fsN5+kds0SLYCn3Vxz4NafvZmaO9l7Fh9679epXW2eeAd6mPh8rbXJMte0Obh+Mzxz/V9+M7VrbaMXntn9zYepXdsn8V2ttXbhbrxp85NHuYa9jV68FXEwGKZ2nQVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgsLKlNrdvv5Sa6358PzyzMU6taotpvBhhrTNI7XpxfJCa+8Gnn4Vnbp4epna90eIHOUmUsbTW2vh+/Dq31tr0p7+K72rx69xaa51bt8Izp69fT+06mY/CM2+/miunOe6eS82NH9wLzwz3c+VW8/PxApLpJ8lCoce5UqzB1SfhmZNruVKswaUL4ZmLf/HV1K69Tx+m5rZ34mU4Xz13N7Xrb/7Xi/DM2na8xOyseKMHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAorGx73fWb11Jzh/efhWdGFzupXa2zFh4ZdHO7Hj57npr7z7/4P+GZL1zOtZP9x/XN8Mwo+a/q6vgoNbf7Try9bvdKvPmrtdY+msRbzabJprybr98Mz9y5mPte04ePU3PnEq1mneU0tasdxn9na92N1KqD8UlqbvHRR+GZ1YNHqV0vtuLPqs0v5BpEb778amru9FH8vroyij9zWmvtK196LTxz++XceZwFb/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoLCypTb7ixepuf5qPzwz6OeOcdqLF5DszcepXbvjXNnJfBX/bgeDXLnH/cEoPLO9mqd2Tbu5udVqEp7ZX+ZKSz57Ei+1Od9dT+16kbhkf3X/r1K7vnDrVmru1Uvx73Z57Xpq1/G9++GZxTh+vVprbbXI3YsvXjxN7Mo9B6br8VKb2X68IKy11qa/fD81N0oUOk3WB6ldd998Kzwze/Db1K6z4I0eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtdcPVMjXXX87CMzvdXAPStBdvrerPpqldJ6e587h15Up45qWXb6d23T9KNPOtcm1cw2RrVWce/8lMl/HGu9Zau3F5JzzTzxWhtYOnj8Izq91cK9+D57mWt/3RMDxzZxL/PbfWWvdZvL2ujXOH353n3rfG8/g5nixyz49VohVxNO6kdj28/1lqbtSJ7zue567Z9iQ+t/P266ldZ8EbPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAorGypzcZ4lJp7ML8QnrnaPU3tujjeC8/0nzxM7ZofvkjNffHNl8Mzd77w+dSu3V+8F5650emldrVBrgxnsIr/b7xxlCtx6bf4ZxyNNlK7fvPhvfDMznHuPeGVz11KzX02jBfUPP4g93vZONwNz3TmuXuqs8jdw6eJUqxpN3fNpsfxXbuLw9Su0eh8au5wGi+POp7krtnu/cfhmf6d66ldZ8EbPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGFl2+v2j+NNV6219t39eEvT/HJqVfvWchqe2XjyKLVrfXaSmvvK174dnrl5+7XUru/86J3wzP4k1xy46Ofuj1miLW9j1UntOv0sfq17l3LNcK9c3AnPnC72U7v6m8PU3Nt/+vXwzG680Owf5n7yJDwzWeaa0Jb9tdTcOHFfbW4mH1Ybm+GR8TDXyre8fDE1d9ri+x49jbcUttba/t6z8MyLX7+f2vWXqanf5Y0eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtddODB6m5D54/Ds+MZ7k2ru2X4o1hXx7kWte2+vFWvtZae/n27fDM+XO5BrXJIt7mNzmJz7TW2nCwSM2druL7ht3c/TGcxq/ZeDfXxtXtxx8Fy16ure3x81wD44t3fxWeGa3nGtQO18/FZzZGqV2Tc1upuePj4/DMaCf329ydxlsiD+e531h3Nk7NPXx0FN+1Hm/la621g1n8ObB5kGt7PAve6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYWVLbf7V3VxZwdPdeJnFjz8+Se36m3vxkoONV3Lfa3RuLTW31YsXdcwO4wUYrbW26MRLMI4nuV3rvdytv+gl/jfu5P6fXnbjc7vH8WKP1lpbncYLdIbHubOf7eWKiFYffhKeGSXfZaaj8+GZd+aT1K57z56k5taX8ZnhMlcYM1iP/146s05q1+lerpjpeBUvB+qfG6R2LQbx73b34nZq11nwRg8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFBY2fa612/mvtq/G90Jz9xeu5/a9T/fizeN/e29WWrXH929mZo7+vDj8Mxe8v/H3jJex7U3zTUHXhnFm65aa22x6oVnZsvcNXu6ip/Hs1G8fbG11k778fa6rU7uN7Z5IXf2y2n8M7bnB6lda2vxlsjPTnPNcM8Xq9Tc9UG8eW20mbs/tjbj57Ea59oNn01z59jvxZ8Fvd3c8+NLq2F45txh7jlwFrzRA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCypbaTJJlJ5fWO+GZP3l9J7Xr2XG8tOQn9/dTu959/CI19/lEUcd0mLutVsv4/52Hp5Pcrkm8lKK11gbr8e+2WuZKS1pibmNtPbXqcBUvIDm4cy216/Jbb6TmevGfS3vnr7+X2nU7cV+9dPFKalebTFNj6/34gezPcoUxx8/jz9PryYKlmzuXU3PDbvy3OdjNPU/vHsYLyW5vb6d2nQVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIWVba/r9HJfrTOPt1bd2M41hv2Lly+EZw6m8Zax1lq7t5dr8zvpxdv8rt6+ndrVG47CM6fzXDPc6eFhaq4/W4RnhoON1K743dHa/PHT1K7zi3l4ZnKQu6d2Z4kautba9sWL8ZlO7l1mcBr/brc2N1O7hsn3rc7mWnxmkPuM3aN4w961fvz33FpriQLR1lpr3Un8t3mSfA5c6MXvj1fv5HLiLHijB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKKxse91qlatAWi0T7WTLeONda629eSl+/E9vnEvtOp7kPuN8HG/L27l8JbVr/Vy8r21vmWuvm01nqbl5Ym7SyzUOdju98Mz55L/umV6t6cF+btlp7jxWj56EZ15quefAoBdv89sa587jai/Xbvgi0Ui5thVvAGytteUsfmPNT/ZSuw4muVbERHldW06OU7tuvHk1PPPyndxz8Sx4oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhZUttVl2cv/DLFq8SKTNcwUpF/rxwo2v3N5J7Xp+uJuamz5+GJ6ZHeeKIoab8XKP0+R1nq1yc91l/FovZom2jdZaZxG/P+bJ85gOMuUv8eKX1lrrzHPnsegN40PdXKnNYh7/bqtkWc/6YpCaW82m4ZlH67mimdla/OyXa6lVbbCZO4+Tk/h5DFfL1K4rd66HZ9b7ifv3jHijB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKKxse91wYzM111sfhWeme0epXZlWs5vb8c/XWmv/bD/XrPXu3uPwzKMHn6R2HYwPwjNHy1z71Gk39z/uYLkKz8xXuba27ir+8zzu5NraTlbxuX7yPWE5yV2z5SR+D3eS7XUtcZ1P+7nrvEw05bXW2nHmM65NUrtaN/7d1ge5+rrlIt5C11prm8v4d3vt2lZq18Vh/OxPnueaA3Of8Hd5oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhZUttWndXmqs0xmEZ/obqVXttDsLzwwSZQqttXbnRq4M5+PP4gUT08lxatdiGd+1N88VYDzr5G79rV78vuqscteskyio2c/1xbRH03hpSbeTe0/oJQp0srJvMoMWv86Pl/Hfc2ut7bdcGc5R4lrfSpb8bCcKuHq7h6ld1/rrqbmv3b4ennn1du7hPRrHi8wmybIepTYAwP+XoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhdVtr1vm/oeZjE/CM9k2rk6iSWo1zTVkndvcTM3tnI83Lu0+fZLadfgoPrffy13nHySbxi4miujOJxoRW2ttM9FeN+vmmvIO5vG502TrWra7rteNX+thom2wtdZGqU+Z29Xv5CoHR4lrvZzNU7umi/h5bCTvjwvncp+xzQ7CI0cvcmd/cD7+m+7Mc8+cndTU7/JGDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZ9rrFMtfitUrMdZINasP+MDyzGucakFruONrVzfhn/Ok7f5/a9fzB0/DMvJO7hZ8mO9QO5vE2v9Ei2U6W+IhryXtxNYxf526iTa611jqJVr7WWuv3441hi1WynWwR/53N57m2tlXyMw4zx59sr1sm7qtuP/fQWbbcM27vaC8801vlzmOtuxWe6Sz/cHHrjR4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFFa21KY7iBdgtNbaINHD0EkWxnR6ieNf5IozFsdHqbkbW6PwzOVB7jMOTsfhmfPLXEHKaSf3P243MTfv50pLjpfxuXHyXmyJEpfePLeskywU6iYKhVarZLlVJ372uW/V2qDTy80lnh8byfv+XGJss5N8DuTGWmvxwcn4OLUp8zgddePP0rPijR4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaCwuu11/dxX660S//uscu1kLdVel2vl63dz3VrnOvHGsD9762Zq1/5JfNfPPnmW2vVsMk/NnS7jbWiTZK/ZMnF/LJP/uy8S36ubrG3sJGveut1sNV9cL9Hy1k9+vI1u7lk16safBVv93OFvdePPuMvJdBklb5BBi/+mh8l7arWI7zpNtHOeFW/0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaCwsqU2bbieHIyXFXRWyTaLRPHOfD5LrVomL3WmvOHGKLWq/eWXb4Vnrg1yhUIfPD5IzT0+jp//i3mupON02QvPTJK34rwTv86rRPFLa611e/Hv1VprvcRcsj+nDRIlP/1kt9VmptyqtbaWOP+1Tu5Dnu8twjMXkwU6m73cfbU+iJ9jP3crttks/hw46cTP8Kx4oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6ACiss8o2rwEA/+R5oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh/xfkBwlHN40TWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f971a122eb8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    \n",
    "    newArray = np.empty_like(x)\n",
    "    newArray = x/255\n",
    "    \n",
    "    return np.array(newArray) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer();\n",
    "encoder = encoder.fit([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    return encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = None\n",
    "    return tf.placeholder(tf.float32, \n",
    "                          shape=(batch_size, image_shape[0], image_shape[1], image_shape[2]),\n",
    "                         name = \"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    batch_size = None\n",
    "    return tf.placeholder(tf.float32, \n",
    "                          shape=(batch_size, n_classes),\n",
    "                         name = \"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    input_shape = x_tensor.get_shape().as_list()\n",
    "    input_depth = input_shape[3]\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], input_depth, conv_num_outputs], \n",
    "                           stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    conv = tf.nn.conv2d(x_tensor, \n",
    "                        weights,\n",
    "                        [1, conv_strides[0], conv_strides[1], 1],\n",
    "                        padding='SAME')\n",
    "    hidden = tf.nn.relu(tf.add(conv, bias))\n",
    "    output = tf.nn.max_pool(hidden, \n",
    "                            ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "                            strides=[1, pool_strides[0], pool_strides[1], 1], \n",
    "                            padding='SAME')\n",
    "    return output \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.contrib.layers.flatten(x_tensor, [x_tensor[1], x_tensor[2], x_tensor[3]])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.truncated_normal([shape[1], num_outputs], stddev=0.1))\n",
    "    biases = tf.Variable(tf.constant(1.0, shape=[num_outputs]))\n",
    "    return tf.add(tf.matmul(x_tensor, weights), biases)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    return fully_conn(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_1 = conv2d_maxpool(x, \n",
    "                            conv_num_outputs=12, \n",
    "                            conv_ksize=(3,3), \n",
    "                            conv_strides=(1,1), \n",
    "                            pool_ksize=(2,2), \n",
    "                            pool_strides=(1,1))\n",
    "\n",
    "    conv_2 = conv2d_maxpool(conv_1, \n",
    "                            conv_num_outputs=24, \n",
    "                            conv_ksize=(4,4), \n",
    "                            conv_strides=(2,2), \n",
    "                            pool_ksize=(3,3), \n",
    "                            pool_strides=(2, 2))\n",
    "    \n",
    "    conv_3 = conv2d_maxpool(conv_2, \n",
    "                            conv_num_outputs=36, \n",
    "                            conv_ksize=(6,6), \n",
    "                            conv_strides=(4,4), \n",
    "                            pool_ksize=(2,2), \n",
    "                            pool_strides=(2,2))\n",
    "    \n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flattened = flatten(conv_3)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc_1 = fully_conn(flattened, 16)\n",
    "    fc_2 = fully_conn(fc_1, 10)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(fc_2, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, \n",
    "                feed_dict={\n",
    "                    x: feature_batch,\n",
    "                    y: label_batch,\n",
    "                    keep_prob: keep_probability})\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # Calculando perda e precisão dos lote\n",
    "    loss = session.run(cost, \n",
    "                       feed_dict={\n",
    "                            x: feature_batch,\n",
    "                            y: label_batch,\n",
    "                            keep_prob: 1.})\n",
    "    valid_acc = session.run(accuracy, \n",
    "                            feed_dict={\n",
    "                                x: valid_features,\n",
    "                                y: valid_labels,\n",
    "                                keep_prob: 1.})\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'\n",
    "          .format(loss,valid_acc))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 300\n",
    "batch_size = 1024\n",
    "keep_probability = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3015 Validation Accuracy: 0.104400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2830 Validation Accuracy: 0.118800\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.2479 Validation Accuracy: 0.147200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.1768 Validation Accuracy: 0.152800\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.1320 Validation Accuracy: 0.200800\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.1131 Validation Accuracy: 0.222800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.0883 Validation Accuracy: 0.231000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.0545 Validation Accuracy: 0.244000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.0333 Validation Accuracy: 0.255600\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.9900 Validation Accuracy: 0.261800\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.9569 Validation Accuracy: 0.280200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.9211 Validation Accuracy: 0.290800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.9038 Validation Accuracy: 0.305600\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.8490 Validation Accuracy: 0.320000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.8161 Validation Accuracy: 0.316200\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.8001 Validation Accuracy: 0.325400\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.7696 Validation Accuracy: 0.335000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.7396 Validation Accuracy: 0.343200\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.7322 Validation Accuracy: 0.352000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.6933 Validation Accuracy: 0.357000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.6725 Validation Accuracy: 0.365000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.6529 Validation Accuracy: 0.368400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.6339 Validation Accuracy: 0.374200\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.6242 Validation Accuracy: 0.381200\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.6122 Validation Accuracy: 0.383600\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.5876 Validation Accuracy: 0.393800\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.5681 Validation Accuracy: 0.396200\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.5525 Validation Accuracy: 0.400200\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.5393 Validation Accuracy: 0.402200\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.5244 Validation Accuracy: 0.404000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.5112 Validation Accuracy: 0.407600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.5061 Validation Accuracy: 0.409600\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.4979 Validation Accuracy: 0.413800\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.4826 Validation Accuracy: 0.416800\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.4691 Validation Accuracy: 0.417400\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.4587 Validation Accuracy: 0.417000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.4484 Validation Accuracy: 0.421600\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.4371 Validation Accuracy: 0.428200\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.4253 Validation Accuracy: 0.433000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.4173 Validation Accuracy: 0.434800\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.4155 Validation Accuracy: 0.438200\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.4101 Validation Accuracy: 0.439400\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.3974 Validation Accuracy: 0.442800\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.3856 Validation Accuracy: 0.443400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.3768 Validation Accuracy: 0.445800\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.3683 Validation Accuracy: 0.448200\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.3600 Validation Accuracy: 0.448000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.3523 Validation Accuracy: 0.450400\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.3439 Validation Accuracy: 0.452400\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.3367 Validation Accuracy: 0.455000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.3313 Validation Accuracy: 0.453200\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.3276 Validation Accuracy: 0.456800\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.3230 Validation Accuracy: 0.457400\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.3156 Validation Accuracy: 0.458000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.3059 Validation Accuracy: 0.460600\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.2970 Validation Accuracy: 0.465200\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.2885 Validation Accuracy: 0.464600\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.2818 Validation Accuracy: 0.465800\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.2750 Validation Accuracy: 0.467400\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.2678 Validation Accuracy: 0.466600\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.2605 Validation Accuracy: 0.467600\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     1.2536 Validation Accuracy: 0.468200\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.2473 Validation Accuracy: 0.470800\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.2412 Validation Accuracy: 0.473200\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     1.2367 Validation Accuracy: 0.472600\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     1.2339 Validation Accuracy: 0.474600\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.2331 Validation Accuracy: 0.474600\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.2328 Validation Accuracy: 0.474800\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     1.2268 Validation Accuracy: 0.475600\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.2140 Validation Accuracy: 0.481600\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.2017 Validation Accuracy: 0.484200\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.1909 Validation Accuracy: 0.484400\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.1829 Validation Accuracy: 0.483800\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.1762 Validation Accuracy: 0.486200\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.1699 Validation Accuracy: 0.487000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.1636 Validation Accuracy: 0.489200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.1564 Validation Accuracy: 0.491600\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.1499 Validation Accuracy: 0.493600\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     1.1445 Validation Accuracy: 0.494400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.1385 Validation Accuracy: 0.496000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     1.1333 Validation Accuracy: 0.497200\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.1281 Validation Accuracy: 0.498400\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.1233 Validation Accuracy: 0.498600\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.1181 Validation Accuracy: 0.500000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.1112 Validation Accuracy: 0.499200\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     1.1044 Validation Accuracy: 0.500800\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     1.0959 Validation Accuracy: 0.502800\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     1.0878 Validation Accuracy: 0.504400\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.0789 Validation Accuracy: 0.508000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     1.0711 Validation Accuracy: 0.508400\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.0633 Validation Accuracy: 0.508000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     1.0564 Validation Accuracy: 0.507400\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     1.0500 Validation Accuracy: 0.509400\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     1.0435 Validation Accuracy: 0.511000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     1.0372 Validation Accuracy: 0.513400\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     1.0317 Validation Accuracy: 0.514600\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     1.0259 Validation Accuracy: 0.515800\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     1.0199 Validation Accuracy: 0.517400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     1.0135 Validation Accuracy: 0.518200\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     1.0067 Validation Accuracy: 0.517400\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     1.0003 Validation Accuracy: 0.518200\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.9937 Validation Accuracy: 0.520200\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.9879 Validation Accuracy: 0.521400\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.9820 Validation Accuracy: 0.524200\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.9768 Validation Accuracy: 0.526400\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.9714 Validation Accuracy: 0.525800\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.9676 Validation Accuracy: 0.528200\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.9635 Validation Accuracy: 0.529800\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.9602 Validation Accuracy: 0.531200\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.9562 Validation Accuracy: 0.532200\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.9506 Validation Accuracy: 0.531000\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.9412 Validation Accuracy: 0.530200\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.9310 Validation Accuracy: 0.531000\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.9232 Validation Accuracy: 0.529800\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.9178 Validation Accuracy: 0.529000\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.9120 Validation Accuracy: 0.530600\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.9067 Validation Accuracy: 0.531000\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.9011 Validation Accuracy: 0.530600\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.8967 Validation Accuracy: 0.528800\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.8922 Validation Accuracy: 0.528000\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.8863 Validation Accuracy: 0.527400\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.8813 Validation Accuracy: 0.527800\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.8759 Validation Accuracy: 0.527600\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.8709 Validation Accuracy: 0.527000\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.8655 Validation Accuracy: 0.527800\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.8609 Validation Accuracy: 0.527800\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.8559 Validation Accuracy: 0.527200\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.8496 Validation Accuracy: 0.529200\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.8446 Validation Accuracy: 0.529800\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.8390 Validation Accuracy: 0.529000\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.8347 Validation Accuracy: 0.529000\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.8274 Validation Accuracy: 0.531200\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.8228 Validation Accuracy: 0.530800\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.8192 Validation Accuracy: 0.531000\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.8115 Validation Accuracy: 0.533000\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.8073 Validation Accuracy: 0.530600\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.8004 Validation Accuracy: 0.533400\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.7982 Validation Accuracy: 0.531000\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.7934 Validation Accuracy: 0.532200\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.7839 Validation Accuracy: 0.534600\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.7803 Validation Accuracy: 0.534400\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.7737 Validation Accuracy: 0.533200\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.7704 Validation Accuracy: 0.534600\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.8021 Validation Accuracy: 0.524800\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.7599 Validation Accuracy: 0.532000\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.7545 Validation Accuracy: 0.536200\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.7529 Validation Accuracy: 0.534400\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.7450 Validation Accuracy: 0.534000\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.7480 Validation Accuracy: 0.532800\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.7625 Validation Accuracy: 0.527200\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.8315 Validation Accuracy: 0.511600\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.7508 Validation Accuracy: 0.529400\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.7423 Validation Accuracy: 0.528600\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.7384 Validation Accuracy: 0.531400\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.7343 Validation Accuracy: 0.529000\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.7108 Validation Accuracy: 0.533200\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.7130 Validation Accuracy: 0.531200\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.7779 Validation Accuracy: 0.517600\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.7250 Validation Accuracy: 0.529400\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.7088 Validation Accuracy: 0.532200\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.6954 Validation Accuracy: 0.532400\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.6944 Validation Accuracy: 0.531200\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.6836 Validation Accuracy: 0.532000\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.6751 Validation Accuracy: 0.531600\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.6727 Validation Accuracy: 0.528200\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.6682 Validation Accuracy: 0.531000\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.6674 Validation Accuracy: 0.529600\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.6633 Validation Accuracy: 0.529200\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.6581 Validation Accuracy: 0.529000\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.6523 Validation Accuracy: 0.528600\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.6475 Validation Accuracy: 0.528200\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.6434 Validation Accuracy: 0.529200\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.6384 Validation Accuracy: 0.530600\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.6366 Validation Accuracy: 0.529000\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.6302 Validation Accuracy: 0.530600\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.6266 Validation Accuracy: 0.528600\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.6234 Validation Accuracy: 0.529400\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.6205 Validation Accuracy: 0.528600\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.6161 Validation Accuracy: 0.527400\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.6113 Validation Accuracy: 0.529200\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.6079 Validation Accuracy: 0.527000\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.6082 Validation Accuracy: 0.527000\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.6041 Validation Accuracy: 0.526200\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.5994 Validation Accuracy: 0.527600\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.5950 Validation Accuracy: 0.527800\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.5921 Validation Accuracy: 0.526000\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.5929 Validation Accuracy: 0.527200\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.5882 Validation Accuracy: 0.527400\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.5848 Validation Accuracy: 0.526200\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.5850 Validation Accuracy: 0.523200\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.5867 Validation Accuracy: 0.522800\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.6065 Validation Accuracy: 0.521800\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.6348 Validation Accuracy: 0.514800\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.5858 Validation Accuracy: 0.530600\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.5778 Validation Accuracy: 0.529000\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.5841 Validation Accuracy: 0.531000\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.5827 Validation Accuracy: 0.526200\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.5642 Validation Accuracy: 0.529400\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.5585 Validation Accuracy: 0.525600\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.5624 Validation Accuracy: 0.524400\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:     0.5710 Validation Accuracy: 0.519800\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss:     0.5727 Validation Accuracy: 0.520400\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss:     0.5672 Validation Accuracy: 0.520800\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss:     0.5657 Validation Accuracy: 0.522200\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss:     0.5715 Validation Accuracy: 0.521200\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss:     0.5820 Validation Accuracy: 0.521200\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss:     0.5959 Validation Accuracy: 0.516200\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss:     0.5963 Validation Accuracy: 0.514000\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss:     0.5382 Validation Accuracy: 0.524600\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss:     0.6963 Validation Accuracy: 0.517400\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss:     0.5441 Validation Accuracy: 0.535800\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss:     0.5421 Validation Accuracy: 0.538600\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss:     0.5204 Validation Accuracy: 0.543600\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss:     0.5278 Validation Accuracy: 0.537200\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss:     0.5236 Validation Accuracy: 0.538200\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss:     0.5072 Validation Accuracy: 0.539800\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss:     0.5067 Validation Accuracy: 0.540400\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss:     0.5015 Validation Accuracy: 0.539800\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss:     0.4955 Validation Accuracy: 0.538000\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss:     0.4909 Validation Accuracy: 0.535800\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss:     0.4919 Validation Accuracy: 0.534400\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss:     0.4912 Validation Accuracy: 0.533600\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss:     0.4903 Validation Accuracy: 0.531600\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss:     0.4885 Validation Accuracy: 0.531000\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss:     0.4896 Validation Accuracy: 0.531400\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss:     0.4880 Validation Accuracy: 0.530200\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss:     0.4874 Validation Accuracy: 0.530600\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss:     0.4855 Validation Accuracy: 0.529800\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss:     0.4852 Validation Accuracy: 0.529400\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss:     0.4868 Validation Accuracy: 0.528600\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss:     0.4871 Validation Accuracy: 0.528200\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss:     0.4854 Validation Accuracy: 0.529600\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss:     0.4873 Validation Accuracy: 0.527800\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss:     0.4873 Validation Accuracy: 0.527600\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss:     0.4895 Validation Accuracy: 0.525400\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss:     0.4866 Validation Accuracy: 0.526600\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss:     0.4826 Validation Accuracy: 0.527200\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss:     0.4760 Validation Accuracy: 0.527400\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss:     0.4697 Validation Accuracy: 0.529600\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss:     0.4682 Validation Accuracy: 0.528000\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss:     0.4836 Validation Accuracy: 0.526400\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss:     0.4974 Validation Accuracy: 0.524200\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss:     0.4928 Validation Accuracy: 0.524000\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss:     0.4760 Validation Accuracy: 0.522400\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss:     0.5107 Validation Accuracy: 0.513400\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss:     0.5532 Validation Accuracy: 0.504200\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss:     0.5159 Validation Accuracy: 0.513200\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss:     0.5430 Validation Accuracy: 0.513200\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss:     0.5806 Validation Accuracy: 0.507000\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss:     0.6194 Validation Accuracy: 0.500600\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss:     0.4602 Validation Accuracy: 0.530000\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss:     0.5838 Validation Accuracy: 0.513400\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss:     0.4668 Validation Accuracy: 0.532800\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss:     0.4643 Validation Accuracy: 0.537400\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss:     0.4403 Validation Accuracy: 0.536000\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss:     0.4415 Validation Accuracy: 0.536000\n",
      "Epoch 257, CIFAR-10 Batch 1:  Loss:     0.4246 Validation Accuracy: 0.533800\n",
      "Epoch 258, CIFAR-10 Batch 1:  Loss:     0.4308 Validation Accuracy: 0.534000\n",
      "Epoch 259, CIFAR-10 Batch 1:  Loss:     0.4225 Validation Accuracy: 0.535600\n",
      "Epoch 260, CIFAR-10 Batch 1:  Loss:     0.4209 Validation Accuracy: 0.534800\n",
      "Epoch 261, CIFAR-10 Batch 1:  Loss:     0.4239 Validation Accuracy: 0.533600\n",
      "Epoch 262, CIFAR-10 Batch 1:  Loss:     0.4212 Validation Accuracy: 0.535200\n",
      "Epoch 263, CIFAR-10 Batch 1:  Loss:     0.4184 Validation Accuracy: 0.533200\n",
      "Epoch 264, CIFAR-10 Batch 1:  Loss:     0.4195 Validation Accuracy: 0.532800\n",
      "Epoch 265, CIFAR-10 Batch 1:  Loss:     0.4199 Validation Accuracy: 0.532200\n",
      "Epoch 266, CIFAR-10 Batch 1:  Loss:     0.4202 Validation Accuracy: 0.530600\n",
      "Epoch 267, CIFAR-10 Batch 1:  Loss:     0.4221 Validation Accuracy: 0.526800\n",
      "Epoch 268, CIFAR-10 Batch 1:  Loss:     0.4251 Validation Accuracy: 0.528400\n",
      "Epoch 269, CIFAR-10 Batch 1:  Loss:     0.4307 Validation Accuracy: 0.527000\n",
      "Epoch 270, CIFAR-10 Batch 1:  Loss:     0.4388 Validation Accuracy: 0.524400\n",
      "Epoch 271, CIFAR-10 Batch 1:  Loss:     0.4496 Validation Accuracy: 0.524600\n",
      "Epoch 272, CIFAR-10 Batch 1:  Loss:     0.4671 Validation Accuracy: 0.520800\n",
      "Epoch 273, CIFAR-10 Batch 1:  Loss:     0.4844 Validation Accuracy: 0.518000\n",
      "Epoch 274, CIFAR-10 Batch 1:  Loss:     0.4918 Validation Accuracy: 0.517600\n",
      "Epoch 275, CIFAR-10 Batch 1:  Loss:     0.4686 Validation Accuracy: 0.523200\n",
      "Epoch 276, CIFAR-10 Batch 1:  Loss:     0.4157 Validation Accuracy: 0.530600\n",
      "Epoch 277, CIFAR-10 Batch 1:  Loss:     0.4561 Validation Accuracy: 0.525600\n",
      "Epoch 278, CIFAR-10 Batch 1:  Loss:     0.4963 Validation Accuracy: 0.522200\n",
      "Epoch 279, CIFAR-10 Batch 1:  Loss:     0.4752 Validation Accuracy: 0.528400\n",
      "Epoch 280, CIFAR-10 Batch 1:  Loss:     0.4056 Validation Accuracy: 0.535600\n",
      "Epoch 281, CIFAR-10 Batch 1:  Loss:     0.3787 Validation Accuracy: 0.540000\n",
      "Epoch 282, CIFAR-10 Batch 1:  Loss:     0.3780 Validation Accuracy: 0.541200\n",
      "Epoch 283, CIFAR-10 Batch 1:  Loss:     0.3867 Validation Accuracy: 0.540400\n",
      "Epoch 284, CIFAR-10 Batch 1:  Loss:     0.3873 Validation Accuracy: 0.539400\n",
      "Epoch 285, CIFAR-10 Batch 1:  Loss:     0.3837 Validation Accuracy: 0.538400\n",
      "Epoch 286, CIFAR-10 Batch 1:  Loss:     0.3821 Validation Accuracy: 0.540600\n",
      "Epoch 287, CIFAR-10 Batch 1:  Loss:     0.3806 Validation Accuracy: 0.540400\n",
      "Epoch 288, CIFAR-10 Batch 1:  Loss:     0.3769 Validation Accuracy: 0.540000\n",
      "Epoch 289, CIFAR-10 Batch 1:  Loss:     0.3737 Validation Accuracy: 0.540400\n",
      "Epoch 290, CIFAR-10 Batch 1:  Loss:     0.3719 Validation Accuracy: 0.539800\n",
      "Epoch 291, CIFAR-10 Batch 1:  Loss:     0.3726 Validation Accuracy: 0.538200\n",
      "Epoch 292, CIFAR-10 Batch 1:  Loss:     0.3714 Validation Accuracy: 0.537200\n",
      "Epoch 293, CIFAR-10 Batch 1:  Loss:     0.3719 Validation Accuracy: 0.534600\n",
      "Epoch 294, CIFAR-10 Batch 1:  Loss:     0.3728 Validation Accuracy: 0.533400\n",
      "Epoch 295, CIFAR-10 Batch 1:  Loss:     0.3755 Validation Accuracy: 0.533000\n",
      "Epoch 296, CIFAR-10 Batch 1:  Loss:     0.3785 Validation Accuracy: 0.530400\n",
      "Epoch 297, CIFAR-10 Batch 1:  Loss:     0.3793 Validation Accuracy: 0.531200\n",
      "Epoch 298, CIFAR-10 Batch 1:  Loss:     0.3759 Validation Accuracy: 0.531400\n",
      "Epoch 299, CIFAR-10 Batch 1:  Loss:     0.3722 Validation Accuracy: 0.530400\n",
      "Epoch 300, CIFAR-10 Batch 1:  Loss:     0.3662 Validation Accuracy: 0.531200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3007 Validation Accuracy: 0.088200\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2918 Validation Accuracy: 0.112400\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2714 Validation Accuracy: 0.125400\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.2268 Validation Accuracy: 0.144600\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.1421 Validation Accuracy: 0.177600\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1335 Validation Accuracy: 0.177200\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.0485 Validation Accuracy: 0.227200\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.0192 Validation Accuracy: 0.246800\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.9832 Validation Accuracy: 0.247200\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.9677 Validation Accuracy: 0.245600\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.9766 Validation Accuracy: 0.266600\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.9181 Validation Accuracy: 0.282400\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.8644 Validation Accuracy: 0.300800\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.8272 Validation Accuracy: 0.296200\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.8196 Validation Accuracy: 0.315200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.8160 Validation Accuracy: 0.328200\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.7746 Validation Accuracy: 0.336400\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.6979 Validation Accuracy: 0.354600\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.6777 Validation Accuracy: 0.365200\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.7000 Validation Accuracy: 0.362200\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.6622 Validation Accuracy: 0.375000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.6601 Validation Accuracy: 0.380600\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.6148 Validation Accuracy: 0.390600\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.5998 Validation Accuracy: 0.392400\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.6227 Validation Accuracy: 0.403400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.5728 Validation Accuracy: 0.408600\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.5899 Validation Accuracy: 0.415800\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.5477 Validation Accuracy: 0.421800\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.5342 Validation Accuracy: 0.427000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.5591 Validation Accuracy: 0.428400\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.5110 Validation Accuracy: 0.433600\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.5314 Validation Accuracy: 0.435600\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.4863 Validation Accuracy: 0.441200\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.4732 Validation Accuracy: 0.438800\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.5093 Validation Accuracy: 0.446800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.4649 Validation Accuracy: 0.448400\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.4819 Validation Accuracy: 0.450000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.4344 Validation Accuracy: 0.457000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.4232 Validation Accuracy: 0.457000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.4674 Validation Accuracy: 0.463400\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.4326 Validation Accuracy: 0.466000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.4333 Validation Accuracy: 0.466600\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.3935 Validation Accuracy: 0.470400\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.3906 Validation Accuracy: 0.468800\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.4401 Validation Accuracy: 0.474800\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.4084 Validation Accuracy: 0.475600\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.4018 Validation Accuracy: 0.476600\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.3674 Validation Accuracy: 0.481800\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.3618 Validation Accuracy: 0.483600\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.4079 Validation Accuracy: 0.483400\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.3898 Validation Accuracy: 0.487200\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.3712 Validation Accuracy: 0.484400\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.3633 Validation Accuracy: 0.485600\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.3250 Validation Accuracy: 0.488800\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.3811 Validation Accuracy: 0.493200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.3647 Validation Accuracy: 0.499000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.3471 Validation Accuracy: 0.497000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.3271 Validation Accuracy: 0.495400\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.3006 Validation Accuracy: 0.499600\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.3567 Validation Accuracy: 0.502600\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.3428 Validation Accuracy: 0.508200\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.3254 Validation Accuracy: 0.503200\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.3028 Validation Accuracy: 0.503000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.2810 Validation Accuracy: 0.502800\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.3338 Validation Accuracy: 0.511200\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.3226 Validation Accuracy: 0.513200\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.3028 Validation Accuracy: 0.510800\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.2854 Validation Accuracy: 0.505000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.2611 Validation Accuracy: 0.512200\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.3158 Validation Accuracy: 0.516400\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.3063 Validation Accuracy: 0.519000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.2823 Validation Accuracy: 0.519400\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.2692 Validation Accuracy: 0.505000\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.2414 Validation Accuracy: 0.518000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.2982 Validation Accuracy: 0.520800\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.2879 Validation Accuracy: 0.524800\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.2649 Validation Accuracy: 0.523800\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.2536 Validation Accuracy: 0.510800\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.2246 Validation Accuracy: 0.520800\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.2789 Validation Accuracy: 0.526800\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.2699 Validation Accuracy: 0.528000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.2470 Validation Accuracy: 0.525800\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.2349 Validation Accuracy: 0.516600\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.2118 Validation Accuracy: 0.524600\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.2610 Validation Accuracy: 0.531200\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.2551 Validation Accuracy: 0.534400\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.2302 Validation Accuracy: 0.532200\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.2186 Validation Accuracy: 0.522000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.2000 Validation Accuracy: 0.531200\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.2446 Validation Accuracy: 0.537800\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.2417 Validation Accuracy: 0.536200\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.2129 Validation Accuracy: 0.537000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.2056 Validation Accuracy: 0.526600\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.1898 Validation Accuracy: 0.534200\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.2307 Validation Accuracy: 0.541400\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.2281 Validation Accuracy: 0.540600\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.1961 Validation Accuracy: 0.543400\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.1955 Validation Accuracy: 0.531200\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.1811 Validation Accuracy: 0.535200\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.2151 Validation Accuracy: 0.541000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.2165 Validation Accuracy: 0.543600\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.1811 Validation Accuracy: 0.547600\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     1.1862 Validation Accuracy: 0.535200\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.1698 Validation Accuracy: 0.538000\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.1993 Validation Accuracy: 0.548800\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.2074 Validation Accuracy: 0.541800\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.1698 Validation Accuracy: 0.549200\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     1.1737 Validation Accuracy: 0.541200\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     1.1571 Validation Accuracy: 0.541600\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.1858 Validation Accuracy: 0.550400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.1993 Validation Accuracy: 0.545600\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.1583 Validation Accuracy: 0.551800\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     1.1604 Validation Accuracy: 0.547600\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.1411 Validation Accuracy: 0.547200\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.1747 Validation Accuracy: 0.554000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.1870 Validation Accuracy: 0.548600\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.1462 Validation Accuracy: 0.555000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     1.1462 Validation Accuracy: 0.551400\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.1291 Validation Accuracy: 0.549800\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.1600 Validation Accuracy: 0.555400\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.1748 Validation Accuracy: 0.552600\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     1.1360 Validation Accuracy: 0.557200\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     1.1358 Validation Accuracy: 0.552800\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     1.1199 Validation Accuracy: 0.553000\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     1.1456 Validation Accuracy: 0.560600\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.1625 Validation Accuracy: 0.557000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     1.1259 Validation Accuracy: 0.561000\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     1.1272 Validation Accuracy: 0.557600\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     1.1105 Validation Accuracy: 0.554200\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     1.1341 Validation Accuracy: 0.562200\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.1524 Validation Accuracy: 0.555800\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     1.1149 Validation Accuracy: 0.564600\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     1.1190 Validation Accuracy: 0.561200\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     1.1012 Validation Accuracy: 0.556400\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     1.1225 Validation Accuracy: 0.566800\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.1420 Validation Accuracy: 0.559600\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     1.1045 Validation Accuracy: 0.565600\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     1.1097 Validation Accuracy: 0.562000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     1.0915 Validation Accuracy: 0.559000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     1.1106 Validation Accuracy: 0.571200\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.1310 Validation Accuracy: 0.566200\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     1.0936 Validation Accuracy: 0.572200\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     1.1007 Validation Accuracy: 0.565200\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     1.0833 Validation Accuracy: 0.559000\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     1.1002 Validation Accuracy: 0.573200\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.1216 Validation Accuracy: 0.571400\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     1.0838 Validation Accuracy: 0.576800\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     1.0933 Validation Accuracy: 0.570200\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     1.0741 Validation Accuracy: 0.562800\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     1.0909 Validation Accuracy: 0.575200\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.1134 Validation Accuracy: 0.574800\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     1.0739 Validation Accuracy: 0.578800\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     1.0854 Validation Accuracy: 0.574600\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     1.0663 Validation Accuracy: 0.565000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     1.0821 Validation Accuracy: 0.577600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.1046 Validation Accuracy: 0.577800\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     1.0648 Validation Accuracy: 0.581000\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     1.0796 Validation Accuracy: 0.578400\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     1.0587 Validation Accuracy: 0.567200\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     1.0735 Validation Accuracy: 0.585000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.0970 Validation Accuracy: 0.581800\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     1.0562 Validation Accuracy: 0.582400\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     1.0708 Validation Accuracy: 0.581200\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     1.0514 Validation Accuracy: 0.567800\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     1.0656 Validation Accuracy: 0.588000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.0888 Validation Accuracy: 0.582200\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     1.0471 Validation Accuracy: 0.585200\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     1.0613 Validation Accuracy: 0.585200\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     1.0427 Validation Accuracy: 0.572000\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     1.0581 Validation Accuracy: 0.591000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.0812 Validation Accuracy: 0.585000\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     1.0391 Validation Accuracy: 0.586800\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     1.0514 Validation Accuracy: 0.587400\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     1.0345 Validation Accuracy: 0.572400\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     1.0507 Validation Accuracy: 0.590200\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.0750 Validation Accuracy: 0.586600\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     1.0303 Validation Accuracy: 0.591000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     1.0429 Validation Accuracy: 0.589800\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     1.0261 Validation Accuracy: 0.576400\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     1.0432 Validation Accuracy: 0.593400\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.0695 Validation Accuracy: 0.591000\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     1.0235 Validation Accuracy: 0.592400\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     1.0338 Validation Accuracy: 0.590800\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     1.0194 Validation Accuracy: 0.578000\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     1.0354 Validation Accuracy: 0.596600\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.0609 Validation Accuracy: 0.592200\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     1.0157 Validation Accuracy: 0.597400\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     1.0252 Validation Accuracy: 0.591400\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     1.0110 Validation Accuracy: 0.580600\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     1.0284 Validation Accuracy: 0.598800\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.0540 Validation Accuracy: 0.592400\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     1.0058 Validation Accuracy: 0.598400\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     1.0165 Validation Accuracy: 0.593400\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     1.0028 Validation Accuracy: 0.582000\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     1.0198 Validation Accuracy: 0.601600\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.0463 Validation Accuracy: 0.597400\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.9984 Validation Accuracy: 0.599600\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     1.0098 Validation Accuracy: 0.594200\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.9960 Validation Accuracy: 0.585400\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     1.0113 Validation Accuracy: 0.602600\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.0379 Validation Accuracy: 0.599200\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.9902 Validation Accuracy: 0.601400\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     1.0018 Validation Accuracy: 0.595400\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.9903 Validation Accuracy: 0.585800\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     1.0034 Validation Accuracy: 0.605200\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.0296 Validation Accuracy: 0.599400\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.9802 Validation Accuracy: 0.603800\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.9942 Validation Accuracy: 0.597200\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.9836 Validation Accuracy: 0.587200\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.9951 Validation Accuracy: 0.605000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.0217 Validation Accuracy: 0.601400\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.9736 Validation Accuracy: 0.605400\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.9865 Validation Accuracy: 0.599600\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.9770 Validation Accuracy: 0.588600\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.9883 Validation Accuracy: 0.606400\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.0146 Validation Accuracy: 0.604800\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.9664 Validation Accuracy: 0.608000\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.9790 Validation Accuracy: 0.599800\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.9702 Validation Accuracy: 0.590800\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.9802 Validation Accuracy: 0.608600\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.0062 Validation Accuracy: 0.604800\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.9606 Validation Accuracy: 0.610800\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.9725 Validation Accuracy: 0.603600\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.9632 Validation Accuracy: 0.592800\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.9724 Validation Accuracy: 0.611000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.9999 Validation Accuracy: 0.607200\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.9549 Validation Accuracy: 0.611800\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.9657 Validation Accuracy: 0.605400\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.9571 Validation Accuracy: 0.593800\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.9637 Validation Accuracy: 0.611600\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.9916 Validation Accuracy: 0.609600\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.9470 Validation Accuracy: 0.613000\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.9588 Validation Accuracy: 0.607200\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.9507 Validation Accuracy: 0.596200\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.9563 Validation Accuracy: 0.612800\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.9845 Validation Accuracy: 0.611000\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.9399 Validation Accuracy: 0.615200\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.9533 Validation Accuracy: 0.608400\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.9470 Validation Accuracy: 0.597000\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.9503 Validation Accuracy: 0.614400\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.9778 Validation Accuracy: 0.611800\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.9334 Validation Accuracy: 0.616200\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.9466 Validation Accuracy: 0.607200\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.9423 Validation Accuracy: 0.598000\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.9437 Validation Accuracy: 0.617600\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.9723 Validation Accuracy: 0.611800\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.9262 Validation Accuracy: 0.616800\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.9413 Validation Accuracy: 0.607800\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.9392 Validation Accuracy: 0.597600\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.9387 Validation Accuracy: 0.618600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.9657 Validation Accuracy: 0.614600\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.9192 Validation Accuracy: 0.617800\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.9351 Validation Accuracy: 0.611200\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.9347 Validation Accuracy: 0.598000\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.9329 Validation Accuracy: 0.618800\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.9593 Validation Accuracy: 0.614800\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.9135 Validation Accuracy: 0.617200\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.9285 Validation Accuracy: 0.612600\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.9318 Validation Accuracy: 0.596600\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.9283 Validation Accuracy: 0.618000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.9530 Validation Accuracy: 0.614200\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.9075 Validation Accuracy: 0.618000\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.9242 Validation Accuracy: 0.615000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.9261 Validation Accuracy: 0.599000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.9233 Validation Accuracy: 0.618600\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.9478 Validation Accuracy: 0.614200\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.9026 Validation Accuracy: 0.619800\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.9173 Validation Accuracy: 0.617200\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.9227 Validation Accuracy: 0.600400\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.9185 Validation Accuracy: 0.619600\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.9429 Validation Accuracy: 0.615600\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.8985 Validation Accuracy: 0.621600\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.9098 Validation Accuracy: 0.618800\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.9174 Validation Accuracy: 0.601400\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.9140 Validation Accuracy: 0.622400\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.9377 Validation Accuracy: 0.617400\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.8938 Validation Accuracy: 0.623200\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.9042 Validation Accuracy: 0.618400\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.9134 Validation Accuracy: 0.601000\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.9090 Validation Accuracy: 0.622000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.9336 Validation Accuracy: 0.618800\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.8900 Validation Accuracy: 0.623200\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.8985 Validation Accuracy: 0.621400\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.9104 Validation Accuracy: 0.602400\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.9035 Validation Accuracy: 0.621200\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.9303 Validation Accuracy: 0.618000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.8873 Validation Accuracy: 0.624200\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.8919 Validation Accuracy: 0.624600\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.9044 Validation Accuracy: 0.602400\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.8978 Validation Accuracy: 0.621600\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.9255 Validation Accuracy: 0.620000\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.8824 Validation Accuracy: 0.624200\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.8868 Validation Accuracy: 0.626400\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.8977 Validation Accuracy: 0.603200\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.8923 Validation Accuracy: 0.623600\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.9222 Validation Accuracy: 0.620400\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.8788 Validation Accuracy: 0.623800\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.8824 Validation Accuracy: 0.627200\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.8899 Validation Accuracy: 0.607400\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.8875 Validation Accuracy: 0.623000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.9171 Validation Accuracy: 0.620000\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.8753 Validation Accuracy: 0.624800\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.8778 Validation Accuracy: 0.629000\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.8846 Validation Accuracy: 0.608800\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.8828 Validation Accuracy: 0.623400\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.9134 Validation Accuracy: 0.621000\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.8711 Validation Accuracy: 0.624200\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.8741 Validation Accuracy: 0.631200\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.8813 Validation Accuracy: 0.608400\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.8794 Validation Accuracy: 0.625400\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.9093 Validation Accuracy: 0.623200\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.8681 Validation Accuracy: 0.624400\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.8716 Validation Accuracy: 0.631200\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.8775 Validation Accuracy: 0.609800\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.8761 Validation Accuracy: 0.625800\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.9053 Validation Accuracy: 0.624000\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.8638 Validation Accuracy: 0.623800\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.8686 Validation Accuracy: 0.632600\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.8731 Validation Accuracy: 0.611200\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.8725 Validation Accuracy: 0.627400\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.9018 Validation Accuracy: 0.623800\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.8595 Validation Accuracy: 0.626400\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.8669 Validation Accuracy: 0.633200\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.8717 Validation Accuracy: 0.609800\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.8689 Validation Accuracy: 0.628600\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.8983 Validation Accuracy: 0.626200\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.8533 Validation Accuracy: 0.625800\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.8703 Validation Accuracy: 0.629800\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.8703 Validation Accuracy: 0.609400\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.8595 Validation Accuracy: 0.630400\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.8889 Validation Accuracy: 0.631800\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.8449 Validation Accuracy: 0.626800\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.8873 Validation Accuracy: 0.625400\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.8639 Validation Accuracy: 0.612600\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.8435 Validation Accuracy: 0.634000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.8808 Validation Accuracy: 0.633200\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.8478 Validation Accuracy: 0.623200\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.8901 Validation Accuracy: 0.622600\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.8389 Validation Accuracy: 0.623000\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.8411 Validation Accuracy: 0.631200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.8821 Validation Accuracy: 0.636400\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.8378 Validation Accuracy: 0.624800\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.8605 Validation Accuracy: 0.632400\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.8327 Validation Accuracy: 0.622200\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.8341 Validation Accuracy: 0.632200\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.8762 Validation Accuracy: 0.634800\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.8333 Validation Accuracy: 0.626600\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.8590 Validation Accuracy: 0.634800\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.8307 Validation Accuracy: 0.619600\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.8287 Validation Accuracy: 0.632600\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.8709 Validation Accuracy: 0.635800\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.8295 Validation Accuracy: 0.627800\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.8571 Validation Accuracy: 0.634600\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.8271 Validation Accuracy: 0.620600\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.8246 Validation Accuracy: 0.632800\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.8669 Validation Accuracy: 0.637800\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.8257 Validation Accuracy: 0.629000\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.8532 Validation Accuracy: 0.637000\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.8230 Validation Accuracy: 0.621400\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.8199 Validation Accuracy: 0.633200\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.8630 Validation Accuracy: 0.638400\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.8225 Validation Accuracy: 0.631200\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.8493 Validation Accuracy: 0.637200\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.8191 Validation Accuracy: 0.622600\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.8166 Validation Accuracy: 0.633800\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.8589 Validation Accuracy: 0.640000\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.8190 Validation Accuracy: 0.630400\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.8464 Validation Accuracy: 0.638600\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.8148 Validation Accuracy: 0.624400\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.8114 Validation Accuracy: 0.633200\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.8544 Validation Accuracy: 0.641200\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.8151 Validation Accuracy: 0.632000\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.8424 Validation Accuracy: 0.639000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.8108 Validation Accuracy: 0.623000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.8061 Validation Accuracy: 0.634600\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.8491 Validation Accuracy: 0.642200\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.8113 Validation Accuracy: 0.633800\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.8396 Validation Accuracy: 0.639800\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.8077 Validation Accuracy: 0.624200\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.8016 Validation Accuracy: 0.634800\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.8453 Validation Accuracy: 0.642600\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.8082 Validation Accuracy: 0.635800\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.8359 Validation Accuracy: 0.640600\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.8036 Validation Accuracy: 0.625200\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.7970 Validation Accuracy: 0.637000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.8411 Validation Accuracy: 0.644000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.8048 Validation Accuracy: 0.635600\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.8338 Validation Accuracy: 0.641200\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.7990 Validation Accuracy: 0.623800\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.7940 Validation Accuracy: 0.637200\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.8371 Validation Accuracy: 0.644600\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.8028 Validation Accuracy: 0.636000\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.8304 Validation Accuracy: 0.640800\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.7961 Validation Accuracy: 0.626000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.7901 Validation Accuracy: 0.637800\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.8314 Validation Accuracy: 0.645200\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.7998 Validation Accuracy: 0.636200\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.8265 Validation Accuracy: 0.641400\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.7918 Validation Accuracy: 0.626800\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.7857 Validation Accuracy: 0.637600\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.8277 Validation Accuracy: 0.644400\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.7978 Validation Accuracy: 0.637200\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.8237 Validation Accuracy: 0.641800\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.7885 Validation Accuracy: 0.628000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.7812 Validation Accuracy: 0.639600\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.8246 Validation Accuracy: 0.642600\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.7945 Validation Accuracy: 0.636600\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.8208 Validation Accuracy: 0.641200\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.7837 Validation Accuracy: 0.628600\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.7780 Validation Accuracy: 0.640400\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.8217 Validation Accuracy: 0.641200\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.7919 Validation Accuracy: 0.638800\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.8195 Validation Accuracy: 0.643600\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.7814 Validation Accuracy: 0.630000\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.7742 Validation Accuracy: 0.640200\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.8184 Validation Accuracy: 0.640800\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.7895 Validation Accuracy: 0.639400\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.8173 Validation Accuracy: 0.642600\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.7781 Validation Accuracy: 0.629400\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.7716 Validation Accuracy: 0.640600\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.8157 Validation Accuracy: 0.640600\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.7868 Validation Accuracy: 0.641800\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.8145 Validation Accuracy: 0.644000\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.7755 Validation Accuracy: 0.629000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.7684 Validation Accuracy: 0.642600\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.8120 Validation Accuracy: 0.643400\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.7837 Validation Accuracy: 0.641000\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.8111 Validation Accuracy: 0.643000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.7707 Validation Accuracy: 0.629800\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.7649 Validation Accuracy: 0.642200\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.8084 Validation Accuracy: 0.645400\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.7805 Validation Accuracy: 0.642200\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.8063 Validation Accuracy: 0.643800\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.7659 Validation Accuracy: 0.630200\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.7605 Validation Accuracy: 0.643200\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.8031 Validation Accuracy: 0.645600\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.7780 Validation Accuracy: 0.641200\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.8046 Validation Accuracy: 0.645000\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.7621 Validation Accuracy: 0.631400\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.7565 Validation Accuracy: 0.643000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.7995 Validation Accuracy: 0.645800\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.7745 Validation Accuracy: 0.642200\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.8012 Validation Accuracy: 0.645400\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.7585 Validation Accuracy: 0.631600\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.7524 Validation Accuracy: 0.642200\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.7962 Validation Accuracy: 0.646600\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.7708 Validation Accuracy: 0.641600\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.7984 Validation Accuracy: 0.644800\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.7540 Validation Accuracy: 0.632600\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.7489 Validation Accuracy: 0.642000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.7919 Validation Accuracy: 0.646800\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.7678 Validation Accuracy: 0.642200\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.7954 Validation Accuracy: 0.645800\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.7482 Validation Accuracy: 0.632000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.7454 Validation Accuracy: 0.643600\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.7888 Validation Accuracy: 0.647400\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.7640 Validation Accuracy: 0.642400\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.7936 Validation Accuracy: 0.647000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.7435 Validation Accuracy: 0.632800\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.7421 Validation Accuracy: 0.642400\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.7856 Validation Accuracy: 0.649000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.7605 Validation Accuracy: 0.644400\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.7922 Validation Accuracy: 0.647000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.7386 Validation Accuracy: 0.634600\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.7385 Validation Accuracy: 0.643200\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.7818 Validation Accuracy: 0.651600\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.7570 Validation Accuracy: 0.644800\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.7902 Validation Accuracy: 0.646600\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.7339 Validation Accuracy: 0.634800\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.7346 Validation Accuracy: 0.643000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.7791 Validation Accuracy: 0.653000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.7544 Validation Accuracy: 0.646600\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.7868 Validation Accuracy: 0.649600\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.7296 Validation Accuracy: 0.635800\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.7311 Validation Accuracy: 0.643600\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.7768 Validation Accuracy: 0.652600\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.7517 Validation Accuracy: 0.647200\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.7846 Validation Accuracy: 0.650200\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.7268 Validation Accuracy: 0.637000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.7275 Validation Accuracy: 0.644800\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.7742 Validation Accuracy: 0.653400\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.7489 Validation Accuracy: 0.647800\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.7829 Validation Accuracy: 0.649000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.7228 Validation Accuracy: 0.637400\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.7242 Validation Accuracy: 0.644000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.7718 Validation Accuracy: 0.653200\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.7461 Validation Accuracy: 0.648400\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.7811 Validation Accuracy: 0.648600\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.7183 Validation Accuracy: 0.637800\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.7210 Validation Accuracy: 0.643800\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.7689 Validation Accuracy: 0.653400\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.7434 Validation Accuracy: 0.647000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.7786 Validation Accuracy: 0.648400\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.7151 Validation Accuracy: 0.637000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.7183 Validation Accuracy: 0.642800\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.7659 Validation Accuracy: 0.654800\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.7403 Validation Accuracy: 0.647800\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.7751 Validation Accuracy: 0.649600\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.7125 Validation Accuracy: 0.637400\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.7150 Validation Accuracy: 0.644200\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.7640 Validation Accuracy: 0.655000\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     0.7368 Validation Accuracy: 0.649000\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     0.7712 Validation Accuracy: 0.650800\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     0.7085 Validation Accuracy: 0.637600\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     0.7104 Validation Accuracy: 0.645800\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.7606 Validation Accuracy: 0.654200\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     0.7346 Validation Accuracy: 0.648400\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     0.7675 Validation Accuracy: 0.651800\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     0.7042 Validation Accuracy: 0.639800\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     0.7078 Validation Accuracy: 0.644200\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.7592 Validation Accuracy: 0.654400\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     0.7310 Validation Accuracy: 0.649400\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     0.7647 Validation Accuracy: 0.651400\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     0.7017 Validation Accuracy: 0.637800\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     0.7038 Validation Accuracy: 0.644800\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.7556 Validation Accuracy: 0.654600\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     0.7283 Validation Accuracy: 0.650200\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     0.7617 Validation Accuracy: 0.652200\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     0.6972 Validation Accuracy: 0.639600\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     0.6992 Validation Accuracy: 0.646800\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.7531 Validation Accuracy: 0.653800\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     0.7257 Validation Accuracy: 0.650800\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     0.7580 Validation Accuracy: 0.652600\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     0.6954 Validation Accuracy: 0.640600\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     0.6963 Validation Accuracy: 0.647000\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.7519 Validation Accuracy: 0.653200\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     0.7225 Validation Accuracy: 0.648000\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     0.7551 Validation Accuracy: 0.651200\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     0.6923 Validation Accuracy: 0.642600\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     0.6929 Validation Accuracy: 0.647200\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.7497 Validation Accuracy: 0.654200\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     0.7188 Validation Accuracy: 0.647800\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     0.7530 Validation Accuracy: 0.649000\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     0.6897 Validation Accuracy: 0.642600\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     0.6899 Validation Accuracy: 0.648400\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.7463 Validation Accuracy: 0.655000\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     0.7164 Validation Accuracy: 0.649800\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     0.7489 Validation Accuracy: 0.649600\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     0.6881 Validation Accuracy: 0.643600\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     0.6858 Validation Accuracy: 0.649400\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.7445 Validation Accuracy: 0.655400\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     0.7134 Validation Accuracy: 0.649400\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     0.7456 Validation Accuracy: 0.649200\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     0.6852 Validation Accuracy: 0.644400\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     0.6825 Validation Accuracy: 0.648800\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.7431 Validation Accuracy: 0.655000\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     0.7104 Validation Accuracy: 0.648200\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     0.7432 Validation Accuracy: 0.650200\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     0.6824 Validation Accuracy: 0.643200\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     0.6783 Validation Accuracy: 0.649200\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.7423 Validation Accuracy: 0.655400\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     0.7080 Validation Accuracy: 0.649400\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     0.7397 Validation Accuracy: 0.648800\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     0.6785 Validation Accuracy: 0.645000\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     0.6745 Validation Accuracy: 0.649200\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.7406 Validation Accuracy: 0.655800\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     0.7057 Validation Accuracy: 0.649400\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     0.7365 Validation Accuracy: 0.648200\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     0.6769 Validation Accuracy: 0.645600\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     0.6726 Validation Accuracy: 0.650000\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.7383 Validation Accuracy: 0.656400\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     0.7039 Validation Accuracy: 0.648000\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     0.7334 Validation Accuracy: 0.650600\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     0.6737 Validation Accuracy: 0.646200\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     0.6685 Validation Accuracy: 0.650200\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.7354 Validation Accuracy: 0.656600\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     0.7013 Validation Accuracy: 0.648200\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     0.7305 Validation Accuracy: 0.649800\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     0.6722 Validation Accuracy: 0.645400\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     0.6644 Validation Accuracy: 0.649200\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.7328 Validation Accuracy: 0.655600\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     0.6996 Validation Accuracy: 0.648600\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     0.7287 Validation Accuracy: 0.651800\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     0.6716 Validation Accuracy: 0.644800\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     0.6620 Validation Accuracy: 0.650800\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.7289 Validation Accuracy: 0.656000\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     0.6976 Validation Accuracy: 0.647800\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     0.7268 Validation Accuracy: 0.651000\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     0.6698 Validation Accuracy: 0.645200\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     0.6579 Validation Accuracy: 0.649600\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.7265 Validation Accuracy: 0.657000\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     0.6958 Validation Accuracy: 0.650800\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     0.7247 Validation Accuracy: 0.651200\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     0.6679 Validation Accuracy: 0.647000\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     0.6553 Validation Accuracy: 0.650400\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.7255 Validation Accuracy: 0.656800\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     0.6922 Validation Accuracy: 0.648400\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     0.7229 Validation Accuracy: 0.650600\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     0.6659 Validation Accuracy: 0.646400\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     0.6520 Validation Accuracy: 0.650400\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.7242 Validation Accuracy: 0.657000\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     0.6898 Validation Accuracy: 0.649600\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     0.7185 Validation Accuracy: 0.650800\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     0.6618 Validation Accuracy: 0.648600\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     0.6495 Validation Accuracy: 0.650800\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.7209 Validation Accuracy: 0.655400\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     0.6877 Validation Accuracy: 0.650600\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     0.7168 Validation Accuracy: 0.650400\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     0.6599 Validation Accuracy: 0.647600\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     0.6462 Validation Accuracy: 0.649400\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.7165 Validation Accuracy: 0.655000\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     0.6862 Validation Accuracy: 0.651000\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     0.7145 Validation Accuracy: 0.648400\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     0.6587 Validation Accuracy: 0.648200\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     0.6426 Validation Accuracy: 0.649800\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.7160 Validation Accuracy: 0.656000\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     0.6845 Validation Accuracy: 0.650200\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     0.7116 Validation Accuracy: 0.649000\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     0.6566 Validation Accuracy: 0.649600\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     0.6403 Validation Accuracy: 0.649400\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.7133 Validation Accuracy: 0.655000\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     0.6813 Validation Accuracy: 0.650600\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     0.7084 Validation Accuracy: 0.650000\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     0.6551 Validation Accuracy: 0.648200\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     0.6373 Validation Accuracy: 0.650800\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.7107 Validation Accuracy: 0.654200\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     0.6803 Validation Accuracy: 0.651200\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     0.7067 Validation Accuracy: 0.649400\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     0.6517 Validation Accuracy: 0.648600\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     0.6330 Validation Accuracy: 0.649000\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.7090 Validation Accuracy: 0.654600\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:     0.6777 Validation Accuracy: 0.651000\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:     0.7023 Validation Accuracy: 0.649800\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:     0.6497 Validation Accuracy: 0.649800\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:     0.6303 Validation Accuracy: 0.650600\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.7053 Validation Accuracy: 0.655600\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:     0.6756 Validation Accuracy: 0.652800\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:     0.7003 Validation Accuracy: 0.649400\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:     0.6471 Validation Accuracy: 0.650000\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:     0.6269 Validation Accuracy: 0.650200\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.7033 Validation Accuracy: 0.655000\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:     0.6738 Validation Accuracy: 0.652200\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:     0.6992 Validation Accuracy: 0.648800\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:     0.6453 Validation Accuracy: 0.650200\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:     0.6248 Validation Accuracy: 0.649400\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.7004 Validation Accuracy: 0.656400\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:     0.6708 Validation Accuracy: 0.653600\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:     0.6964 Validation Accuracy: 0.650000\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:     0.6442 Validation Accuracy: 0.648400\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:     0.6223 Validation Accuracy: 0.649400\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.6979 Validation Accuracy: 0.656000\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:     0.6688 Validation Accuracy: 0.652800\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:     0.6948 Validation Accuracy: 0.651400\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:     0.6427 Validation Accuracy: 0.649600\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:     0.6192 Validation Accuracy: 0.647600\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.6957 Validation Accuracy: 0.655800\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:     0.6665 Validation Accuracy: 0.653600\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:     0.6929 Validation Accuracy: 0.651400\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:     0.6399 Validation Accuracy: 0.649000\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:     0.6168 Validation Accuracy: 0.648200\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.6937 Validation Accuracy: 0.655800\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:     0.6639 Validation Accuracy: 0.652800\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:     0.6905 Validation Accuracy: 0.650800\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:     0.6367 Validation Accuracy: 0.650600\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:     0.6134 Validation Accuracy: 0.648200\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.6923 Validation Accuracy: 0.655200\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:     0.6622 Validation Accuracy: 0.654600\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:     0.6855 Validation Accuracy: 0.651400\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:     0.6348 Validation Accuracy: 0.652200\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:     0.6103 Validation Accuracy: 0.650800\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.6917 Validation Accuracy: 0.653600\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:     0.6593 Validation Accuracy: 0.652400\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:     0.6826 Validation Accuracy: 0.652200\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:     0.6348 Validation Accuracy: 0.649000\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:     0.6076 Validation Accuracy: 0.650800\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.6910 Validation Accuracy: 0.655200\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:     0.6582 Validation Accuracy: 0.651600\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:     0.6786 Validation Accuracy: 0.651400\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:     0.6334 Validation Accuracy: 0.649800\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:     0.6059 Validation Accuracy: 0.651600\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.6881 Validation Accuracy: 0.653600\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:     0.6569 Validation Accuracy: 0.651800\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:     0.6754 Validation Accuracy: 0.652000\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:     0.6312 Validation Accuracy: 0.649200\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:     0.6038 Validation Accuracy: 0.650400\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.6844 Validation Accuracy: 0.654600\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:     0.6557 Validation Accuracy: 0.652600\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:     0.6741 Validation Accuracy: 0.651600\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:     0.6301 Validation Accuracy: 0.648400\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:     0.6021 Validation Accuracy: 0.651200\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.6812 Validation Accuracy: 0.655600\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:     0.6535 Validation Accuracy: 0.651400\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:     0.6727 Validation Accuracy: 0.651600\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:     0.6302 Validation Accuracy: 0.648600\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:     0.5986 Validation Accuracy: 0.651200\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.6796 Validation Accuracy: 0.654800\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:     0.6503 Validation Accuracy: 0.651400\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:     0.6737 Validation Accuracy: 0.651400\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:     0.6306 Validation Accuracy: 0.649400\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:     0.5966 Validation Accuracy: 0.651600\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.6776 Validation Accuracy: 0.654600\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:     0.6510 Validation Accuracy: 0.652200\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:     0.6722 Validation Accuracy: 0.650000\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:     0.6315 Validation Accuracy: 0.648800\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:     0.5944 Validation Accuracy: 0.650800\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.6765 Validation Accuracy: 0.654600\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:     0.6494 Validation Accuracy: 0.651000\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:     0.6700 Validation Accuracy: 0.649000\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:     0.6289 Validation Accuracy: 0.649400\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:     0.5902 Validation Accuracy: 0.650400\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.6751 Validation Accuracy: 0.655000\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:     0.6454 Validation Accuracy: 0.652000\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:     0.6639 Validation Accuracy: 0.653000\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:     0.6264 Validation Accuracy: 0.652600\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:     0.5878 Validation Accuracy: 0.653400\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.6784 Validation Accuracy: 0.652200\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:     0.6446 Validation Accuracy: 0.650800\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:     0.6571 Validation Accuracy: 0.653200\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:     0.6230 Validation Accuracy: 0.652600\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:     0.5862 Validation Accuracy: 0.651200\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.6736 Validation Accuracy: 0.651200\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:     0.6468 Validation Accuracy: 0.649000\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:     0.6515 Validation Accuracy: 0.655800\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:     0.6212 Validation Accuracy: 0.653200\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:     0.5849 Validation Accuracy: 0.651000\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.6665 Validation Accuracy: 0.651000\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:     0.6495 Validation Accuracy: 0.646400\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:     0.6517 Validation Accuracy: 0.657600\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:     0.6215 Validation Accuracy: 0.651800\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:     0.5821 Validation Accuracy: 0.651000\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.6632 Validation Accuracy: 0.653000\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:     0.6497 Validation Accuracy: 0.646200\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:     0.6580 Validation Accuracy: 0.658600\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:     0.6187 Validation Accuracy: 0.651400\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:     0.5812 Validation Accuracy: 0.649600\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.6608 Validation Accuracy: 0.653200\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:     0.6418 Validation Accuracy: 0.649600\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:     0.6617 Validation Accuracy: 0.653800\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:     0.6162 Validation Accuracy: 0.649000\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:     0.5807 Validation Accuracy: 0.648200\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.6585 Validation Accuracy: 0.653200\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:     0.6372 Validation Accuracy: 0.652000\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:     0.6572 Validation Accuracy: 0.654200\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:     0.6118 Validation Accuracy: 0.649600\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:     0.5769 Validation Accuracy: 0.648600\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.6565 Validation Accuracy: 0.652600\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:     0.6334 Validation Accuracy: 0.652000\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:     0.6513 Validation Accuracy: 0.656200\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:     0.6085 Validation Accuracy: 0.650400\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:     0.5740 Validation Accuracy: 0.649200\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.6558 Validation Accuracy: 0.654400\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:     0.6315 Validation Accuracy: 0.651600\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:     0.6472 Validation Accuracy: 0.656600\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:     0.6070 Validation Accuracy: 0.649400\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:     0.5714 Validation Accuracy: 0.648400\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.6529 Validation Accuracy: 0.654600\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:     0.6309 Validation Accuracy: 0.651800\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:     0.6461 Validation Accuracy: 0.654400\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:     0.6062 Validation Accuracy: 0.649200\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:     0.5696 Validation Accuracy: 0.647600\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.6516 Validation Accuracy: 0.653800\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:     0.6312 Validation Accuracy: 0.650200\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:     0.6463 Validation Accuracy: 0.652600\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:     0.6062 Validation Accuracy: 0.651200\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:     0.5675 Validation Accuracy: 0.648200\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.6488 Validation Accuracy: 0.653400\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:     0.6328 Validation Accuracy: 0.650000\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:     0.6489 Validation Accuracy: 0.652200\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:     0.6044 Validation Accuracy: 0.650200\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:     0.5692 Validation Accuracy: 0.646800\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.6454 Validation Accuracy: 0.655400\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:     0.6363 Validation Accuracy: 0.648800\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:     0.6575 Validation Accuracy: 0.649600\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:     0.6007 Validation Accuracy: 0.651600\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:     0.5742 Validation Accuracy: 0.645000\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.6466 Validation Accuracy: 0.656200\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:     0.6340 Validation Accuracy: 0.649000\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:     0.6740 Validation Accuracy: 0.644400\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:     0.5987 Validation Accuracy: 0.652400\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:     0.5694 Validation Accuracy: 0.646600\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.6572 Validation Accuracy: 0.654600\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:     0.6292 Validation Accuracy: 0.645800\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:     0.6719 Validation Accuracy: 0.646200\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:     0.5987 Validation Accuracy: 0.649200\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:     0.5593 Validation Accuracy: 0.644800\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.6471 Validation Accuracy: 0.659600\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:     0.6313 Validation Accuracy: 0.646800\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:     0.6733 Validation Accuracy: 0.647400\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:     0.6065 Validation Accuracy: 0.642400\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:     0.5649 Validation Accuracy: 0.646000\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.6413 Validation Accuracy: 0.658400\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:     0.6395 Validation Accuracy: 0.648600\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:     0.6548 Validation Accuracy: 0.650200\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:     0.6404 Validation Accuracy: 0.629800\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:     0.5985 Validation Accuracy: 0.646800\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.6787 Validation Accuracy: 0.638800\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:     0.6470 Validation Accuracy: 0.654200\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:     0.6341 Validation Accuracy: 0.650000\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:     0.6189 Validation Accuracy: 0.640600\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:     0.5689 Validation Accuracy: 0.656800\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.6471 Validation Accuracy: 0.649800\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:     0.6191 Validation Accuracy: 0.655800\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:     0.6382 Validation Accuracy: 0.649400\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:     0.6122 Validation Accuracy: 0.638800\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:     0.5765 Validation Accuracy: 0.653600\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.6492 Validation Accuracy: 0.648200\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:     0.6170 Validation Accuracy: 0.656000\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:     0.6350 Validation Accuracy: 0.649600\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:     0.6180 Validation Accuracy: 0.637200\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:     0.5773 Validation Accuracy: 0.649600\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.6525 Validation Accuracy: 0.646200\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:     0.6160 Validation Accuracy: 0.654600\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:     0.6403 Validation Accuracy: 0.645600\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:     0.6265 Validation Accuracy: 0.636800\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:     0.5748 Validation Accuracy: 0.651000\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.6461 Validation Accuracy: 0.647000\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:     0.6128 Validation Accuracy: 0.655600\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:     0.6387 Validation Accuracy: 0.647800\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:     0.6159 Validation Accuracy: 0.639400\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:     0.5678 Validation Accuracy: 0.651600\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.6344 Validation Accuracy: 0.648200\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:     0.6135 Validation Accuracy: 0.654000\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:     0.6371 Validation Accuracy: 0.646600\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:     0.6060 Validation Accuracy: 0.641800\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:     0.5666 Validation Accuracy: 0.652600\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.6284 Validation Accuracy: 0.650000\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:     0.6173 Validation Accuracy: 0.653400\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:     0.6276 Validation Accuracy: 0.646400\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:     0.6086 Validation Accuracy: 0.642000\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:     0.5748 Validation Accuracy: 0.649000\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.6252 Validation Accuracy: 0.651200\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:     0.6180 Validation Accuracy: 0.654200\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:     0.6182 Validation Accuracy: 0.650200\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:     0.6212 Validation Accuracy: 0.636800\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:     0.5741 Validation Accuracy: 0.649000\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.6279 Validation Accuracy: 0.648600\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:     0.6139 Validation Accuracy: 0.655400\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:     0.6365 Validation Accuracy: 0.643600\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:     0.6091 Validation Accuracy: 0.642000\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:     0.5899 Validation Accuracy: 0.645400\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.6279 Validation Accuracy: 0.651400\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:     0.6081 Validation Accuracy: 0.653200\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:     0.6351 Validation Accuracy: 0.647600\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:     0.5879 Validation Accuracy: 0.647000\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:     0.5841 Validation Accuracy: 0.644200\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.6402 Validation Accuracy: 0.651000\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:     0.6014 Validation Accuracy: 0.655400\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:     0.6260 Validation Accuracy: 0.649800\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:     0.5920 Validation Accuracy: 0.641800\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:     0.5711 Validation Accuracy: 0.646400\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.6489 Validation Accuracy: 0.649000\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:     0.5985 Validation Accuracy: 0.658400\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:     0.6257 Validation Accuracy: 0.650000\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:     0.5964 Validation Accuracy: 0.640000\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:     0.5609 Validation Accuracy: 0.650000\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.6559 Validation Accuracy: 0.647600\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:     0.6039 Validation Accuracy: 0.652400\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:     0.6239 Validation Accuracy: 0.648800\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:     0.6013 Validation Accuracy: 0.638000\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:     0.5534 Validation Accuracy: 0.655000\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.6537 Validation Accuracy: 0.649400\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:     0.6217 Validation Accuracy: 0.647400\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:     0.6282 Validation Accuracy: 0.652800\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:     0.6009 Validation Accuracy: 0.641000\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:     0.5552 Validation Accuracy: 0.653000\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.6281 Validation Accuracy: 0.651000\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:     0.6257 Validation Accuracy: 0.648400\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:     0.6430 Validation Accuracy: 0.647600\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:     0.6060 Validation Accuracy: 0.640600\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:     0.5572 Validation Accuracy: 0.652200\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.6162 Validation Accuracy: 0.653000\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:     0.6101 Validation Accuracy: 0.653200\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:     0.6284 Validation Accuracy: 0.651000\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:     0.6146 Validation Accuracy: 0.638200\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:     0.5685 Validation Accuracy: 0.650800\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.6224 Validation Accuracy: 0.647600\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:     0.6173 Validation Accuracy: 0.655800\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:     0.6158 Validation Accuracy: 0.651000\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:     0.6015 Validation Accuracy: 0.642600\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:     0.5609 Validation Accuracy: 0.653800\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.6189 Validation Accuracy: 0.647600\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:     0.6127 Validation Accuracy: 0.654400\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:     0.6140 Validation Accuracy: 0.652000\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:     0.5969 Validation Accuracy: 0.642200\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:     0.5553 Validation Accuracy: 0.655000\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.6159 Validation Accuracy: 0.649400\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:     0.6069 Validation Accuracy: 0.655200\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:     0.6139 Validation Accuracy: 0.655000\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:     0.5946 Validation Accuracy: 0.642200\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:     0.5514 Validation Accuracy: 0.655600\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.6132 Validation Accuracy: 0.649000\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:     0.6028 Validation Accuracy: 0.655000\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:     0.6121 Validation Accuracy: 0.654600\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:     0.5914 Validation Accuracy: 0.643400\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:     0.5477 Validation Accuracy: 0.654600\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.6111 Validation Accuracy: 0.647600\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:     0.5988 Validation Accuracy: 0.655000\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:     0.6110 Validation Accuracy: 0.653000\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:     0.5899 Validation Accuracy: 0.642200\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:     0.5464 Validation Accuracy: 0.654400\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.6111 Validation Accuracy: 0.647200\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:     0.5979 Validation Accuracy: 0.656000\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:     0.6092 Validation Accuracy: 0.653600\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:     0.5862 Validation Accuracy: 0.643200\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:     0.5432 Validation Accuracy: 0.657000\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.6095 Validation Accuracy: 0.647000\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:     0.5953 Validation Accuracy: 0.656600\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:     0.6077 Validation Accuracy: 0.652800\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:     0.5847 Validation Accuracy: 0.642000\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:     0.5414 Validation Accuracy: 0.657200\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.6082 Validation Accuracy: 0.646800\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:     0.5947 Validation Accuracy: 0.657000\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:     0.6068 Validation Accuracy: 0.650800\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:     0.5839 Validation Accuracy: 0.642400\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:     0.5394 Validation Accuracy: 0.655200\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.6069 Validation Accuracy: 0.645600\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:     0.5912 Validation Accuracy: 0.656800\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:     0.6050 Validation Accuracy: 0.650400\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:     0.5829 Validation Accuracy: 0.641400\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:     0.5370 Validation Accuracy: 0.654600\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.6048 Validation Accuracy: 0.646200\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:     0.5898 Validation Accuracy: 0.657600\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:     0.6041 Validation Accuracy: 0.650000\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:     0.5825 Validation Accuracy: 0.641400\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:     0.5353 Validation Accuracy: 0.654000\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.6032 Validation Accuracy: 0.645800\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:     0.5860 Validation Accuracy: 0.656400\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:     0.6013 Validation Accuracy: 0.651400\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:     0.5812 Validation Accuracy: 0.642000\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:     0.5338 Validation Accuracy: 0.653000\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.6012 Validation Accuracy: 0.647200\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:     0.5832 Validation Accuracy: 0.656000\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:     0.5991 Validation Accuracy: 0.651600\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:     0.5777 Validation Accuracy: 0.641000\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:     0.5309 Validation Accuracy: 0.653600\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.5997 Validation Accuracy: 0.646400\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:     0.5804 Validation Accuracy: 0.656000\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:     0.5974 Validation Accuracy: 0.651000\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:     0.5759 Validation Accuracy: 0.642600\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:     0.5282 Validation Accuracy: 0.653200\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.5978 Validation Accuracy: 0.646600\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:     0.5772 Validation Accuracy: 0.654600\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:     0.5943 Validation Accuracy: 0.652000\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:     0.5742 Validation Accuracy: 0.644000\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:     0.5259 Validation Accuracy: 0.653400\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.5953 Validation Accuracy: 0.647200\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:     0.5750 Validation Accuracy: 0.655000\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:     0.5921 Validation Accuracy: 0.651400\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:     0.5732 Validation Accuracy: 0.644200\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:     0.5237 Validation Accuracy: 0.652400\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.5934 Validation Accuracy: 0.646800\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:     0.5734 Validation Accuracy: 0.655600\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:     0.5900 Validation Accuracy: 0.653000\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:     0.5705 Validation Accuracy: 0.643800\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:     0.5219 Validation Accuracy: 0.652200\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.5913 Validation Accuracy: 0.647200\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:     0.5718 Validation Accuracy: 0.654800\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:     0.5893 Validation Accuracy: 0.651800\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:     0.5698 Validation Accuracy: 0.643400\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:     0.5194 Validation Accuracy: 0.652600\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.5892 Validation Accuracy: 0.647200\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:     0.5694 Validation Accuracy: 0.654000\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:     0.5871 Validation Accuracy: 0.652800\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:     0.5687 Validation Accuracy: 0.643400\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:     0.5184 Validation Accuracy: 0.652000\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.5884 Validation Accuracy: 0.645600\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:     0.5681 Validation Accuracy: 0.654000\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:     0.5835 Validation Accuracy: 0.652200\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:     0.5661 Validation Accuracy: 0.644600\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:     0.5167 Validation Accuracy: 0.652000\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.5863 Validation Accuracy: 0.646800\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:     0.5664 Validation Accuracy: 0.653600\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:     0.5817 Validation Accuracy: 0.651800\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:     0.5641 Validation Accuracy: 0.643400\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:     0.5148 Validation Accuracy: 0.651000\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.5850 Validation Accuracy: 0.646400\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:     0.5651 Validation Accuracy: 0.654000\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:     0.5792 Validation Accuracy: 0.651800\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:     0.5626 Validation Accuracy: 0.643000\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:     0.5131 Validation Accuracy: 0.653000\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.5829 Validation Accuracy: 0.647000\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:     0.5620 Validation Accuracy: 0.654400\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:     0.5756 Validation Accuracy: 0.653600\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:     0.5599 Validation Accuracy: 0.642800\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:     0.5121 Validation Accuracy: 0.652000\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.5823 Validation Accuracy: 0.645000\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:     0.5610 Validation Accuracy: 0.653600\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:     0.5746 Validation Accuracy: 0.654000\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:     0.5582 Validation Accuracy: 0.643400\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:     0.5103 Validation Accuracy: 0.651600\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.5797 Validation Accuracy: 0.645200\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:     0.5597 Validation Accuracy: 0.652600\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:     0.5713 Validation Accuracy: 0.654400\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:     0.5553 Validation Accuracy: 0.643800\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:     0.5086 Validation Accuracy: 0.651200\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.5782 Validation Accuracy: 0.645400\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:     0.5578 Validation Accuracy: 0.653400\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:     0.5683 Validation Accuracy: 0.655600\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:     0.5530 Validation Accuracy: 0.643000\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:     0.5054 Validation Accuracy: 0.649600\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.5762 Validation Accuracy: 0.645800\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:     0.5553 Validation Accuracy: 0.653600\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:     0.5667 Validation Accuracy: 0.656400\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:     0.5499 Validation Accuracy: 0.643600\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:     0.5031 Validation Accuracy: 0.650000\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.5758 Validation Accuracy: 0.646600\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:     0.5537 Validation Accuracy: 0.652000\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:     0.5647 Validation Accuracy: 0.657600\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:     0.5484 Validation Accuracy: 0.645600\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:     0.5003 Validation Accuracy: 0.649000\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:     0.5720 Validation Accuracy: 0.647600\n",
      "Epoch 201, CIFAR-10 Batch 2:  Loss:     0.5525 Validation Accuracy: 0.652400\n",
      "Epoch 201, CIFAR-10 Batch 3:  Loss:     0.5614 Validation Accuracy: 0.656000\n",
      "Epoch 201, CIFAR-10 Batch 4:  Loss:     0.5445 Validation Accuracy: 0.646600\n",
      "Epoch 201, CIFAR-10 Batch 5:  Loss:     0.4978 Validation Accuracy: 0.650400\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss:     0.5716 Validation Accuracy: 0.646600\n",
      "Epoch 202, CIFAR-10 Batch 2:  Loss:     0.5498 Validation Accuracy: 0.652400\n",
      "Epoch 202, CIFAR-10 Batch 3:  Loss:     0.5604 Validation Accuracy: 0.655800\n",
      "Epoch 202, CIFAR-10 Batch 4:  Loss:     0.5435 Validation Accuracy: 0.644800\n",
      "Epoch 202, CIFAR-10 Batch 5:  Loss:     0.4954 Validation Accuracy: 0.650400\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss:     0.5695 Validation Accuracy: 0.647000\n",
      "Epoch 203, CIFAR-10 Batch 2:  Loss:     0.5474 Validation Accuracy: 0.651400\n",
      "Epoch 203, CIFAR-10 Batch 3:  Loss:     0.5582 Validation Accuracy: 0.656000\n",
      "Epoch 203, CIFAR-10 Batch 4:  Loss:     0.5409 Validation Accuracy: 0.645400\n",
      "Epoch 203, CIFAR-10 Batch 5:  Loss:     0.4928 Validation Accuracy: 0.652000\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss:     0.5682 Validation Accuracy: 0.646800\n",
      "Epoch 204, CIFAR-10 Batch 2:  Loss:     0.5448 Validation Accuracy: 0.651400\n",
      "Epoch 204, CIFAR-10 Batch 3:  Loss:     0.5557 Validation Accuracy: 0.657400\n",
      "Epoch 204, CIFAR-10 Batch 4:  Loss:     0.5377 Validation Accuracy: 0.645000\n",
      "Epoch 204, CIFAR-10 Batch 5:  Loss:     0.4901 Validation Accuracy: 0.652000\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss:     0.5673 Validation Accuracy: 0.648600\n",
      "Epoch 205, CIFAR-10 Batch 2:  Loss:     0.5420 Validation Accuracy: 0.651000\n",
      "Epoch 205, CIFAR-10 Batch 3:  Loss:     0.5546 Validation Accuracy: 0.656600\n",
      "Epoch 205, CIFAR-10 Batch 4:  Loss:     0.5353 Validation Accuracy: 0.644400\n",
      "Epoch 205, CIFAR-10 Batch 5:  Loss:     0.4873 Validation Accuracy: 0.652200\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss:     0.5671 Validation Accuracy: 0.648200\n",
      "Epoch 206, CIFAR-10 Batch 2:  Loss:     0.5402 Validation Accuracy: 0.651000\n",
      "Epoch 206, CIFAR-10 Batch 3:  Loss:     0.5525 Validation Accuracy: 0.656000\n",
      "Epoch 206, CIFAR-10 Batch 4:  Loss:     0.5329 Validation Accuracy: 0.645800\n",
      "Epoch 206, CIFAR-10 Batch 5:  Loss:     0.4860 Validation Accuracy: 0.651200\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss:     0.5656 Validation Accuracy: 0.649200\n",
      "Epoch 207, CIFAR-10 Batch 2:  Loss:     0.5391 Validation Accuracy: 0.651200\n",
      "Epoch 207, CIFAR-10 Batch 3:  Loss:     0.5496 Validation Accuracy: 0.655800\n",
      "Epoch 207, CIFAR-10 Batch 4:  Loss:     0.5308 Validation Accuracy: 0.646400\n",
      "Epoch 207, CIFAR-10 Batch 5:  Loss:     0.4837 Validation Accuracy: 0.651200\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss:     0.5647 Validation Accuracy: 0.649800\n",
      "Epoch 208, CIFAR-10 Batch 2:  Loss:     0.5370 Validation Accuracy: 0.651800\n",
      "Epoch 208, CIFAR-10 Batch 3:  Loss:     0.5486 Validation Accuracy: 0.656200\n",
      "Epoch 208, CIFAR-10 Batch 4:  Loss:     0.5285 Validation Accuracy: 0.647400\n",
      "Epoch 208, CIFAR-10 Batch 5:  Loss:     0.4810 Validation Accuracy: 0.649200\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss:     0.5635 Validation Accuracy: 0.649600\n",
      "Epoch 209, CIFAR-10 Batch 2:  Loss:     0.5350 Validation Accuracy: 0.650600\n",
      "Epoch 209, CIFAR-10 Batch 3:  Loss:     0.5480 Validation Accuracy: 0.655600\n",
      "Epoch 209, CIFAR-10 Batch 4:  Loss:     0.5266 Validation Accuracy: 0.646800\n",
      "Epoch 209, CIFAR-10 Batch 5:  Loss:     0.4785 Validation Accuracy: 0.650800\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss:     0.5628 Validation Accuracy: 0.649200\n",
      "Epoch 210, CIFAR-10 Batch 2:  Loss:     0.5340 Validation Accuracy: 0.651000\n",
      "Epoch 210, CIFAR-10 Batch 3:  Loss:     0.5470 Validation Accuracy: 0.655200\n",
      "Epoch 210, CIFAR-10 Batch 4:  Loss:     0.5245 Validation Accuracy: 0.647400\n",
      "Epoch 210, CIFAR-10 Batch 5:  Loss:     0.4753 Validation Accuracy: 0.650400\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss:     0.5594 Validation Accuracy: 0.647600\n",
      "Epoch 211, CIFAR-10 Batch 2:  Loss:     0.5326 Validation Accuracy: 0.650400\n",
      "Epoch 211, CIFAR-10 Batch 3:  Loss:     0.5456 Validation Accuracy: 0.654800\n",
      "Epoch 211, CIFAR-10 Batch 4:  Loss:     0.5245 Validation Accuracy: 0.646400\n",
      "Epoch 211, CIFAR-10 Batch 5:  Loss:     0.4739 Validation Accuracy: 0.652200\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss:     0.5575 Validation Accuracy: 0.649800\n",
      "Epoch 212, CIFAR-10 Batch 2:  Loss:     0.5308 Validation Accuracy: 0.651600\n",
      "Epoch 212, CIFAR-10 Batch 3:  Loss:     0.5457 Validation Accuracy: 0.655600\n",
      "Epoch 212, CIFAR-10 Batch 4:  Loss:     0.5223 Validation Accuracy: 0.647600\n",
      "Epoch 212, CIFAR-10 Batch 5:  Loss:     0.4725 Validation Accuracy: 0.651600\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss:     0.5560 Validation Accuracy: 0.649400\n",
      "Epoch 213, CIFAR-10 Batch 2:  Loss:     0.5304 Validation Accuracy: 0.650400\n",
      "Epoch 213, CIFAR-10 Batch 3:  Loss:     0.5427 Validation Accuracy: 0.656200\n",
      "Epoch 213, CIFAR-10 Batch 4:  Loss:     0.5201 Validation Accuracy: 0.647200\n",
      "Epoch 213, CIFAR-10 Batch 5:  Loss:     0.4695 Validation Accuracy: 0.653200\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss:     0.5535 Validation Accuracy: 0.649600\n",
      "Epoch 214, CIFAR-10 Batch 2:  Loss:     0.5279 Validation Accuracy: 0.651000\n",
      "Epoch 214, CIFAR-10 Batch 3:  Loss:     0.5393 Validation Accuracy: 0.656000\n",
      "Epoch 214, CIFAR-10 Batch 4:  Loss:     0.5194 Validation Accuracy: 0.647800\n",
      "Epoch 214, CIFAR-10 Batch 5:  Loss:     0.4680 Validation Accuracy: 0.651800\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss:     0.5509 Validation Accuracy: 0.650200\n",
      "Epoch 215, CIFAR-10 Batch 2:  Loss:     0.5255 Validation Accuracy: 0.651000\n",
      "Epoch 215, CIFAR-10 Batch 3:  Loss:     0.5380 Validation Accuracy: 0.655600\n",
      "Epoch 215, CIFAR-10 Batch 4:  Loss:     0.5180 Validation Accuracy: 0.647400\n",
      "Epoch 215, CIFAR-10 Batch 5:  Loss:     0.4659 Validation Accuracy: 0.652600\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss:     0.5495 Validation Accuracy: 0.648800\n",
      "Epoch 216, CIFAR-10 Batch 2:  Loss:     0.5229 Validation Accuracy: 0.652800\n",
      "Epoch 216, CIFAR-10 Batch 3:  Loss:     0.5358 Validation Accuracy: 0.656200\n",
      "Epoch 216, CIFAR-10 Batch 4:  Loss:     0.5177 Validation Accuracy: 0.646800\n",
      "Epoch 216, CIFAR-10 Batch 5:  Loss:     0.4648 Validation Accuracy: 0.653200\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss:     0.5466 Validation Accuracy: 0.647400\n",
      "Epoch 217, CIFAR-10 Batch 2:  Loss:     0.5210 Validation Accuracy: 0.654200\n",
      "Epoch 217, CIFAR-10 Batch 3:  Loss:     0.5342 Validation Accuracy: 0.655800\n",
      "Epoch 217, CIFAR-10 Batch 4:  Loss:     0.5167 Validation Accuracy: 0.647600\n",
      "Epoch 217, CIFAR-10 Batch 5:  Loss:     0.4637 Validation Accuracy: 0.652400\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss:     0.5441 Validation Accuracy: 0.649400\n",
      "Epoch 218, CIFAR-10 Batch 2:  Loss:     0.5197 Validation Accuracy: 0.654200\n",
      "Epoch 218, CIFAR-10 Batch 3:  Loss:     0.5327 Validation Accuracy: 0.656000\n",
      "Epoch 218, CIFAR-10 Batch 4:  Loss:     0.5148 Validation Accuracy: 0.649200\n",
      "Epoch 218, CIFAR-10 Batch 5:  Loss:     0.4624 Validation Accuracy: 0.653200\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss:     0.5422 Validation Accuracy: 0.649800\n",
      "Epoch 219, CIFAR-10 Batch 2:  Loss:     0.5166 Validation Accuracy: 0.652400\n",
      "Epoch 219, CIFAR-10 Batch 3:  Loss:     0.5314 Validation Accuracy: 0.656200\n",
      "Epoch 219, CIFAR-10 Batch 4:  Loss:     0.5147 Validation Accuracy: 0.647600\n",
      "Epoch 219, CIFAR-10 Batch 5:  Loss:     0.4622 Validation Accuracy: 0.652000\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss:     0.5403 Validation Accuracy: 0.650400\n",
      "Epoch 220, CIFAR-10 Batch 2:  Loss:     0.5143 Validation Accuracy: 0.653800\n",
      "Epoch 220, CIFAR-10 Batch 3:  Loss:     0.5278 Validation Accuracy: 0.655800\n",
      "Epoch 220, CIFAR-10 Batch 4:  Loss:     0.5112 Validation Accuracy: 0.647600\n",
      "Epoch 220, CIFAR-10 Batch 5:  Loss:     0.4597 Validation Accuracy: 0.654000\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss:     0.5396 Validation Accuracy: 0.650200\n",
      "Epoch 221, CIFAR-10 Batch 2:  Loss:     0.5126 Validation Accuracy: 0.653000\n",
      "Epoch 221, CIFAR-10 Batch 3:  Loss:     0.5260 Validation Accuracy: 0.657000\n",
      "Epoch 221, CIFAR-10 Batch 4:  Loss:     0.5113 Validation Accuracy: 0.647400\n",
      "Epoch 221, CIFAR-10 Batch 5:  Loss:     0.4595 Validation Accuracy: 0.652200\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss:     0.5375 Validation Accuracy: 0.650200\n",
      "Epoch 222, CIFAR-10 Batch 2:  Loss:     0.5099 Validation Accuracy: 0.652200\n",
      "Epoch 222, CIFAR-10 Batch 3:  Loss:     0.5254 Validation Accuracy: 0.656400\n",
      "Epoch 222, CIFAR-10 Batch 4:  Loss:     0.5095 Validation Accuracy: 0.647000\n",
      "Epoch 222, CIFAR-10 Batch 5:  Loss:     0.4584 Validation Accuracy: 0.651800\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss:     0.5355 Validation Accuracy: 0.648800\n",
      "Epoch 223, CIFAR-10 Batch 2:  Loss:     0.5085 Validation Accuracy: 0.654400\n",
      "Epoch 223, CIFAR-10 Batch 3:  Loss:     0.5252 Validation Accuracy: 0.656800\n",
      "Epoch 223, CIFAR-10 Batch 4:  Loss:     0.5078 Validation Accuracy: 0.647600\n",
      "Epoch 223, CIFAR-10 Batch 5:  Loss:     0.4566 Validation Accuracy: 0.651400\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss:     0.5354 Validation Accuracy: 0.650400\n",
      "Epoch 224, CIFAR-10 Batch 2:  Loss:     0.5061 Validation Accuracy: 0.652800\n",
      "Epoch 224, CIFAR-10 Batch 3:  Loss:     0.5241 Validation Accuracy: 0.656400\n",
      "Epoch 224, CIFAR-10 Batch 4:  Loss:     0.5075 Validation Accuracy: 0.647000\n",
      "Epoch 224, CIFAR-10 Batch 5:  Loss:     0.4559 Validation Accuracy: 0.650600\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss:     0.5330 Validation Accuracy: 0.649600\n",
      "Epoch 225, CIFAR-10 Batch 2:  Loss:     0.5046 Validation Accuracy: 0.653000\n",
      "Epoch 225, CIFAR-10 Batch 3:  Loss:     0.5241 Validation Accuracy: 0.656200\n",
      "Epoch 225, CIFAR-10 Batch 4:  Loss:     0.5044 Validation Accuracy: 0.648600\n",
      "Epoch 225, CIFAR-10 Batch 5:  Loss:     0.4547 Validation Accuracy: 0.651000\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss:     0.5320 Validation Accuracy: 0.650800\n",
      "Epoch 226, CIFAR-10 Batch 2:  Loss:     0.5044 Validation Accuracy: 0.654000\n",
      "Epoch 226, CIFAR-10 Batch 3:  Loss:     0.5233 Validation Accuracy: 0.656200\n",
      "Epoch 226, CIFAR-10 Batch 4:  Loss:     0.5044 Validation Accuracy: 0.647600\n",
      "Epoch 226, CIFAR-10 Batch 5:  Loss:     0.4517 Validation Accuracy: 0.651000\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss:     0.5305 Validation Accuracy: 0.653600\n",
      "Epoch 227, CIFAR-10 Batch 2:  Loss:     0.5029 Validation Accuracy: 0.653000\n",
      "Epoch 227, CIFAR-10 Batch 3:  Loss:     0.5235 Validation Accuracy: 0.658400\n",
      "Epoch 227, CIFAR-10 Batch 4:  Loss:     0.5039 Validation Accuracy: 0.645800\n",
      "Epoch 227, CIFAR-10 Batch 5:  Loss:     0.4504 Validation Accuracy: 0.650600\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss:     0.5310 Validation Accuracy: 0.651600\n",
      "Epoch 228, CIFAR-10 Batch 2:  Loss:     0.5016 Validation Accuracy: 0.651000\n",
      "Epoch 228, CIFAR-10 Batch 3:  Loss:     0.5245 Validation Accuracy: 0.657400\n",
      "Epoch 228, CIFAR-10 Batch 4:  Loss:     0.5037 Validation Accuracy: 0.645000\n",
      "Epoch 228, CIFAR-10 Batch 5:  Loss:     0.4503 Validation Accuracy: 0.650800\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss:     0.5287 Validation Accuracy: 0.651400\n",
      "Epoch 229, CIFAR-10 Batch 2:  Loss:     0.5019 Validation Accuracy: 0.651200\n",
      "Epoch 229, CIFAR-10 Batch 3:  Loss:     0.5262 Validation Accuracy: 0.656600\n",
      "Epoch 229, CIFAR-10 Batch 4:  Loss:     0.5027 Validation Accuracy: 0.647400\n",
      "Epoch 229, CIFAR-10 Batch 5:  Loss:     0.4475 Validation Accuracy: 0.651800\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss:     0.5264 Validation Accuracy: 0.651000\n",
      "Epoch 230, CIFAR-10 Batch 2:  Loss:     0.5015 Validation Accuracy: 0.652200\n",
      "Epoch 230, CIFAR-10 Batch 3:  Loss:     0.5270 Validation Accuracy: 0.656800\n",
      "Epoch 230, CIFAR-10 Batch 4:  Loss:     0.4982 Validation Accuracy: 0.648800\n",
      "Epoch 230, CIFAR-10 Batch 5:  Loss:     0.4462 Validation Accuracy: 0.652800\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss:     0.5252 Validation Accuracy: 0.649800\n",
      "Epoch 231, CIFAR-10 Batch 2:  Loss:     0.5008 Validation Accuracy: 0.653000\n",
      "Epoch 231, CIFAR-10 Batch 3:  Loss:     0.5264 Validation Accuracy: 0.657400\n",
      "Epoch 231, CIFAR-10 Batch 4:  Loss:     0.4945 Validation Accuracy: 0.648000\n",
      "Epoch 231, CIFAR-10 Batch 5:  Loss:     0.4471 Validation Accuracy: 0.651200\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss:     0.5226 Validation Accuracy: 0.649200\n",
      "Epoch 232, CIFAR-10 Batch 2:  Loss:     0.4985 Validation Accuracy: 0.655400\n",
      "Epoch 232, CIFAR-10 Batch 3:  Loss:     0.5251 Validation Accuracy: 0.657000\n",
      "Epoch 232, CIFAR-10 Batch 4:  Loss:     0.4945 Validation Accuracy: 0.646800\n",
      "Epoch 232, CIFAR-10 Batch 5:  Loss:     0.4457 Validation Accuracy: 0.650200\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss:     0.5237 Validation Accuracy: 0.650200\n",
      "Epoch 233, CIFAR-10 Batch 2:  Loss:     0.4961 Validation Accuracy: 0.654800\n",
      "Epoch 233, CIFAR-10 Batch 3:  Loss:     0.5265 Validation Accuracy: 0.656000\n",
      "Epoch 233, CIFAR-10 Batch 4:  Loss:     0.4947 Validation Accuracy: 0.644600\n",
      "Epoch 233, CIFAR-10 Batch 5:  Loss:     0.4456 Validation Accuracy: 0.650400\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss:     0.5218 Validation Accuracy: 0.650400\n",
      "Epoch 234, CIFAR-10 Batch 2:  Loss:     0.4967 Validation Accuracy: 0.655200\n",
      "Epoch 234, CIFAR-10 Batch 3:  Loss:     0.5304 Validation Accuracy: 0.656400\n",
      "Epoch 234, CIFAR-10 Batch 4:  Loss:     0.4959 Validation Accuracy: 0.645000\n",
      "Epoch 234, CIFAR-10 Batch 5:  Loss:     0.4438 Validation Accuracy: 0.649600\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss:     0.5226 Validation Accuracy: 0.651200\n",
      "Epoch 235, CIFAR-10 Batch 2:  Loss:     0.5025 Validation Accuracy: 0.653600\n",
      "Epoch 235, CIFAR-10 Batch 3:  Loss:     0.5283 Validation Accuracy: 0.654600\n",
      "Epoch 235, CIFAR-10 Batch 4:  Loss:     0.4957 Validation Accuracy: 0.648200\n",
      "Epoch 235, CIFAR-10 Batch 5:  Loss:     0.4423 Validation Accuracy: 0.648600\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss:     0.5240 Validation Accuracy: 0.651600\n",
      "Epoch 236, CIFAR-10 Batch 2:  Loss:     0.5066 Validation Accuracy: 0.653400\n",
      "Epoch 236, CIFAR-10 Batch 3:  Loss:     0.5213 Validation Accuracy: 0.655800\n",
      "Epoch 236, CIFAR-10 Batch 4:  Loss:     0.5013 Validation Accuracy: 0.646600\n",
      "Epoch 236, CIFAR-10 Batch 5:  Loss:     0.4457 Validation Accuracy: 0.648000\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss:     0.5228 Validation Accuracy: 0.650400\n",
      "Epoch 237, CIFAR-10 Batch 2:  Loss:     0.5043 Validation Accuracy: 0.654400\n",
      "Epoch 237, CIFAR-10 Batch 3:  Loss:     0.5188 Validation Accuracy: 0.654400\n",
      "Epoch 237, CIFAR-10 Batch 4:  Loss:     0.5082 Validation Accuracy: 0.644200\n",
      "Epoch 237, CIFAR-10 Batch 5:  Loss:     0.4506 Validation Accuracy: 0.649000\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss:     0.5224 Validation Accuracy: 0.652000\n",
      "Epoch 238, CIFAR-10 Batch 2:  Loss:     0.5035 Validation Accuracy: 0.654200\n",
      "Epoch 238, CIFAR-10 Batch 3:  Loss:     0.5199 Validation Accuracy: 0.654400\n",
      "Epoch 238, CIFAR-10 Batch 4:  Loss:     0.5119 Validation Accuracy: 0.644200\n",
      "Epoch 238, CIFAR-10 Batch 5:  Loss:     0.4510 Validation Accuracy: 0.650400\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss:     0.5235 Validation Accuracy: 0.652400\n",
      "Epoch 239, CIFAR-10 Batch 2:  Loss:     0.5013 Validation Accuracy: 0.652600\n",
      "Epoch 239, CIFAR-10 Batch 3:  Loss:     0.5112 Validation Accuracy: 0.653400\n",
      "Epoch 239, CIFAR-10 Batch 4:  Loss:     0.5045 Validation Accuracy: 0.648800\n",
      "Epoch 239, CIFAR-10 Batch 5:  Loss:     0.4461 Validation Accuracy: 0.651000\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss:     0.5171 Validation Accuracy: 0.653400\n",
      "Epoch 240, CIFAR-10 Batch 2:  Loss:     0.4973 Validation Accuracy: 0.653200\n",
      "Epoch 240, CIFAR-10 Batch 3:  Loss:     0.5069 Validation Accuracy: 0.658400\n",
      "Epoch 240, CIFAR-10 Batch 4:  Loss:     0.4985 Validation Accuracy: 0.650000\n",
      "Epoch 240, CIFAR-10 Batch 5:  Loss:     0.4404 Validation Accuracy: 0.650000\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss:     0.5160 Validation Accuracy: 0.655600\n",
      "Epoch 241, CIFAR-10 Batch 2:  Loss:     0.4928 Validation Accuracy: 0.650400\n",
      "Epoch 241, CIFAR-10 Batch 3:  Loss:     0.5086 Validation Accuracy: 0.657200\n",
      "Epoch 241, CIFAR-10 Batch 4:  Loss:     0.4965 Validation Accuracy: 0.647800\n",
      "Epoch 241, CIFAR-10 Batch 5:  Loss:     0.4335 Validation Accuracy: 0.651000\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss:     0.5161 Validation Accuracy: 0.651200\n",
      "Epoch 242, CIFAR-10 Batch 2:  Loss:     0.4926 Validation Accuracy: 0.652000\n",
      "Epoch 242, CIFAR-10 Batch 3:  Loss:     0.5105 Validation Accuracy: 0.655000\n",
      "Epoch 242, CIFAR-10 Batch 4:  Loss:     0.4936 Validation Accuracy: 0.647800\n",
      "Epoch 242, CIFAR-10 Batch 5:  Loss:     0.4320 Validation Accuracy: 0.649800\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss:     0.5188 Validation Accuracy: 0.649400\n",
      "Epoch 243, CIFAR-10 Batch 2:  Loss:     0.4945 Validation Accuracy: 0.649600\n",
      "Epoch 243, CIFAR-10 Batch 3:  Loss:     0.5123 Validation Accuracy: 0.655600\n",
      "Epoch 243, CIFAR-10 Batch 4:  Loss:     0.4911 Validation Accuracy: 0.646400\n",
      "Epoch 243, CIFAR-10 Batch 5:  Loss:     0.4318 Validation Accuracy: 0.650200\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss:     0.5209 Validation Accuracy: 0.650000\n",
      "Epoch 244, CIFAR-10 Batch 2:  Loss:     0.4939 Validation Accuracy: 0.648600\n",
      "Epoch 244, CIFAR-10 Batch 3:  Loss:     0.5107 Validation Accuracy: 0.654800\n",
      "Epoch 244, CIFAR-10 Batch 4:  Loss:     0.4841 Validation Accuracy: 0.646800\n",
      "Epoch 244, CIFAR-10 Batch 5:  Loss:     0.4348 Validation Accuracy: 0.653400\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss:     0.5194 Validation Accuracy: 0.648800\n",
      "Epoch 245, CIFAR-10 Batch 2:  Loss:     0.4923 Validation Accuracy: 0.647400\n",
      "Epoch 245, CIFAR-10 Batch 3:  Loss:     0.5053 Validation Accuracy: 0.653400\n",
      "Epoch 245, CIFAR-10 Batch 4:  Loss:     0.4755 Validation Accuracy: 0.651400\n",
      "Epoch 245, CIFAR-10 Batch 5:  Loss:     0.4511 Validation Accuracy: 0.652800\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss:     0.5201 Validation Accuracy: 0.649200\n",
      "Epoch 246, CIFAR-10 Batch 2:  Loss:     0.4945 Validation Accuracy: 0.650600\n",
      "Epoch 246, CIFAR-10 Batch 3:  Loss:     0.5161 Validation Accuracy: 0.651600\n",
      "Epoch 246, CIFAR-10 Batch 4:  Loss:     0.4817 Validation Accuracy: 0.652200\n",
      "Epoch 246, CIFAR-10 Batch 5:  Loss:     0.4383 Validation Accuracy: 0.656000\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss:     0.5197 Validation Accuracy: 0.646200\n",
      "Epoch 247, CIFAR-10 Batch 2:  Loss:     0.5018 Validation Accuracy: 0.651200\n",
      "Epoch 247, CIFAR-10 Batch 3:  Loss:     0.5224 Validation Accuracy: 0.655400\n",
      "Epoch 247, CIFAR-10 Batch 4:  Loss:     0.4877 Validation Accuracy: 0.649400\n",
      "Epoch 247, CIFAR-10 Batch 5:  Loss:     0.4301 Validation Accuracy: 0.656600\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss:     0.5184 Validation Accuracy: 0.644000\n",
      "Epoch 248, CIFAR-10 Batch 2:  Loss:     0.4912 Validation Accuracy: 0.655000\n",
      "Epoch 248, CIFAR-10 Batch 3:  Loss:     0.5226 Validation Accuracy: 0.657000\n",
      "Epoch 248, CIFAR-10 Batch 4:  Loss:     0.4913 Validation Accuracy: 0.644200\n",
      "Epoch 248, CIFAR-10 Batch 5:  Loss:     0.4307 Validation Accuracy: 0.651400\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss:     0.5139 Validation Accuracy: 0.644800\n",
      "Epoch 249, CIFAR-10 Batch 2:  Loss:     0.4898 Validation Accuracy: 0.653800\n",
      "Epoch 249, CIFAR-10 Batch 3:  Loss:     0.5169 Validation Accuracy: 0.657800\n",
      "Epoch 249, CIFAR-10 Batch 4:  Loss:     0.4946 Validation Accuracy: 0.640400\n",
      "Epoch 249, CIFAR-10 Batch 5:  Loss:     0.4300 Validation Accuracy: 0.651200\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss:     0.5156 Validation Accuracy: 0.645400\n",
      "Epoch 250, CIFAR-10 Batch 2:  Loss:     0.4926 Validation Accuracy: 0.652200\n",
      "Epoch 250, CIFAR-10 Batch 3:  Loss:     0.5125 Validation Accuracy: 0.657600\n",
      "Epoch 250, CIFAR-10 Batch 4:  Loss:     0.4978 Validation Accuracy: 0.639800\n",
      "Epoch 250, CIFAR-10 Batch 5:  Loss:     0.4290 Validation Accuracy: 0.654400\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss:     0.5200 Validation Accuracy: 0.644000\n",
      "Epoch 251, CIFAR-10 Batch 2:  Loss:     0.4940 Validation Accuracy: 0.652600\n",
      "Epoch 251, CIFAR-10 Batch 3:  Loss:     0.5125 Validation Accuracy: 0.654600\n",
      "Epoch 251, CIFAR-10 Batch 4:  Loss:     0.4996 Validation Accuracy: 0.639800\n",
      "Epoch 251, CIFAR-10 Batch 5:  Loss:     0.4284 Validation Accuracy: 0.653200\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss:     0.5276 Validation Accuracy: 0.641200\n",
      "Epoch 252, CIFAR-10 Batch 2:  Loss:     0.4996 Validation Accuracy: 0.650800\n",
      "Epoch 252, CIFAR-10 Batch 3:  Loss:     0.5164 Validation Accuracy: 0.651400\n",
      "Epoch 252, CIFAR-10 Batch 4:  Loss:     0.4997 Validation Accuracy: 0.635600\n",
      "Epoch 252, CIFAR-10 Batch 5:  Loss:     0.4372 Validation Accuracy: 0.650200\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss:     0.5257 Validation Accuracy: 0.641000\n",
      "Epoch 253, CIFAR-10 Batch 2:  Loss:     0.5249 Validation Accuracy: 0.643800\n",
      "Epoch 253, CIFAR-10 Batch 3:  Loss:     0.5310 Validation Accuracy: 0.644800\n",
      "Epoch 253, CIFAR-10 Batch 4:  Loss:     0.5136 Validation Accuracy: 0.627600\n",
      "Epoch 253, CIFAR-10 Batch 5:  Loss:     0.4763 Validation Accuracy: 0.647800\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss:     0.5229 Validation Accuracy: 0.635600\n",
      "Epoch 254, CIFAR-10 Batch 2:  Loss:     0.5324 Validation Accuracy: 0.642600\n",
      "Epoch 254, CIFAR-10 Batch 3:  Loss:     0.5242 Validation Accuracy: 0.645800\n",
      "Epoch 254, CIFAR-10 Batch 4:  Loss:     0.5420 Validation Accuracy: 0.627800\n",
      "Epoch 254, CIFAR-10 Batch 5:  Loss:     0.4758 Validation Accuracy: 0.650800\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss:     0.5400 Validation Accuracy: 0.634400\n",
      "Epoch 255, CIFAR-10 Batch 2:  Loss:     0.5155 Validation Accuracy: 0.644800\n",
      "Epoch 255, CIFAR-10 Batch 3:  Loss:     0.5227 Validation Accuracy: 0.645400\n",
      "Epoch 255, CIFAR-10 Batch 4:  Loss:     0.5258 Validation Accuracy: 0.631400\n",
      "Epoch 255, CIFAR-10 Batch 5:  Loss:     0.4462 Validation Accuracy: 0.652000\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss:     0.5116 Validation Accuracy: 0.643600\n",
      "Epoch 256, CIFAR-10 Batch 2:  Loss:     0.5077 Validation Accuracy: 0.647400\n",
      "Epoch 256, CIFAR-10 Batch 3:  Loss:     0.5356 Validation Accuracy: 0.648600\n",
      "Epoch 256, CIFAR-10 Batch 4:  Loss:     0.5391 Validation Accuracy: 0.626800\n",
      "Epoch 256, CIFAR-10 Batch 5:  Loss:     0.4517 Validation Accuracy: 0.649600\n",
      "Epoch 257, CIFAR-10 Batch 1:  Loss:     0.5078 Validation Accuracy: 0.650400\n",
      "Epoch 257, CIFAR-10 Batch 2:  Loss:     0.5348 Validation Accuracy: 0.638600\n",
      "Epoch 257, CIFAR-10 Batch 3:  Loss:     0.5289 Validation Accuracy: 0.645600\n",
      "Epoch 257, CIFAR-10 Batch 4:  Loss:     0.5431 Validation Accuracy: 0.632400\n",
      "Epoch 257, CIFAR-10 Batch 5:  Loss:     0.4896 Validation Accuracy: 0.633000\n",
      "Epoch 258, CIFAR-10 Batch 1:  Loss:     0.5393 Validation Accuracy: 0.644600\n",
      "Epoch 258, CIFAR-10 Batch 2:  Loss:     0.5191 Validation Accuracy: 0.649800\n",
      "Epoch 258, CIFAR-10 Batch 3:  Loss:     0.5207 Validation Accuracy: 0.643200\n",
      "Epoch 258, CIFAR-10 Batch 4:  Loss:     0.5440 Validation Accuracy: 0.631200\n",
      "Epoch 258, CIFAR-10 Batch 5:  Loss:     0.4939 Validation Accuracy: 0.630800\n",
      "Epoch 259, CIFAR-10 Batch 1:  Loss:     0.5560 Validation Accuracy: 0.635200\n",
      "Epoch 259, CIFAR-10 Batch 2:  Loss:     0.5279 Validation Accuracy: 0.643400\n",
      "Epoch 259, CIFAR-10 Batch 3:  Loss:     0.5321 Validation Accuracy: 0.641200\n",
      "Epoch 259, CIFAR-10 Batch 4:  Loss:     0.5460 Validation Accuracy: 0.629600\n",
      "Epoch 259, CIFAR-10 Batch 5:  Loss:     0.5025 Validation Accuracy: 0.634000\n",
      "Epoch 260, CIFAR-10 Batch 1:  Loss:     0.5472 Validation Accuracy: 0.639800\n",
      "Epoch 260, CIFAR-10 Batch 2:  Loss:     0.5444 Validation Accuracy: 0.634800\n",
      "Epoch 260, CIFAR-10 Batch 3:  Loss:     0.5497 Validation Accuracy: 0.640600\n",
      "Epoch 260, CIFAR-10 Batch 4:  Loss:     0.5322 Validation Accuracy: 0.635000\n",
      "Epoch 260, CIFAR-10 Batch 5:  Loss:     0.4675 Validation Accuracy: 0.641200\n",
      "Epoch 261, CIFAR-10 Batch 1:  Loss:     0.5452 Validation Accuracy: 0.649600\n",
      "Epoch 261, CIFAR-10 Batch 2:  Loss:     0.5821 Validation Accuracy: 0.626800\n",
      "Epoch 261, CIFAR-10 Batch 3:  Loss:     0.5290 Validation Accuracy: 0.645200\n",
      "Epoch 261, CIFAR-10 Batch 4:  Loss:     0.5258 Validation Accuracy: 0.653600\n",
      "Epoch 261, CIFAR-10 Batch 5:  Loss:     0.4901 Validation Accuracy: 0.634800\n",
      "Epoch 262, CIFAR-10 Batch 1:  Loss:     0.5219 Validation Accuracy: 0.654200\n",
      "Epoch 262, CIFAR-10 Batch 2:  Loss:     0.5578 Validation Accuracy: 0.641200\n",
      "Epoch 262, CIFAR-10 Batch 3:  Loss:     0.5244 Validation Accuracy: 0.641200\n",
      "Epoch 262, CIFAR-10 Batch 4:  Loss:     0.4968 Validation Accuracy: 0.651800\n",
      "Epoch 262, CIFAR-10 Batch 5:  Loss:     0.4679 Validation Accuracy: 0.643400\n",
      "Epoch 263, CIFAR-10 Batch 1:  Loss:     0.5186 Validation Accuracy: 0.652400\n",
      "Epoch 263, CIFAR-10 Batch 2:  Loss:     0.5210 Validation Accuracy: 0.642800\n",
      "Epoch 263, CIFAR-10 Batch 3:  Loss:     0.5342 Validation Accuracy: 0.639000\n",
      "Epoch 263, CIFAR-10 Batch 4:  Loss:     0.5016 Validation Accuracy: 0.652200\n",
      "Epoch 263, CIFAR-10 Batch 5:  Loss:     0.4766 Validation Accuracy: 0.643200\n",
      "Epoch 264, CIFAR-10 Batch 1:  Loss:     0.5065 Validation Accuracy: 0.653600\n",
      "Epoch 264, CIFAR-10 Batch 2:  Loss:     0.4887 Validation Accuracy: 0.645800\n",
      "Epoch 264, CIFAR-10 Batch 3:  Loss:     0.5529 Validation Accuracy: 0.629400\n",
      "Epoch 264, CIFAR-10 Batch 4:  Loss:     0.5362 Validation Accuracy: 0.638200\n",
      "Epoch 264, CIFAR-10 Batch 5:  Loss:     0.4561 Validation Accuracy: 0.646200\n",
      "Epoch 265, CIFAR-10 Batch 1:  Loss:     0.5137 Validation Accuracy: 0.651400\n",
      "Epoch 265, CIFAR-10 Batch 2:  Loss:     0.4888 Validation Accuracy: 0.647400\n",
      "Epoch 265, CIFAR-10 Batch 3:  Loss:     0.5075 Validation Accuracy: 0.645800\n",
      "Epoch 265, CIFAR-10 Batch 4:  Loss:     0.5583 Validation Accuracy: 0.630800\n",
      "Epoch 265, CIFAR-10 Batch 5:  Loss:     0.4766 Validation Accuracy: 0.641600\n",
      "Epoch 266, CIFAR-10 Batch 1:  Loss:     0.5133 Validation Accuracy: 0.653000\n",
      "Epoch 266, CIFAR-10 Batch 2:  Loss:     0.5131 Validation Accuracy: 0.637600\n",
      "Epoch 266, CIFAR-10 Batch 3:  Loss:     0.5011 Validation Accuracy: 0.651400\n",
      "Epoch 266, CIFAR-10 Batch 4:  Loss:     0.5038 Validation Accuracy: 0.652600\n",
      "Epoch 266, CIFAR-10 Batch 5:  Loss:     0.4743 Validation Accuracy: 0.641400\n",
      "Epoch 267, CIFAR-10 Batch 1:  Loss:     0.5234 Validation Accuracy: 0.651400\n",
      "Epoch 267, CIFAR-10 Batch 2:  Loss:     0.5504 Validation Accuracy: 0.632400\n",
      "Epoch 267, CIFAR-10 Batch 3:  Loss:     0.5214 Validation Accuracy: 0.650000\n",
      "Epoch 267, CIFAR-10 Batch 4:  Loss:     0.4910 Validation Accuracy: 0.657800\n",
      "Epoch 267, CIFAR-10 Batch 5:  Loss:     0.4481 Validation Accuracy: 0.645200\n",
      "Epoch 268, CIFAR-10 Batch 1:  Loss:     0.5042 Validation Accuracy: 0.656800\n",
      "Epoch 268, CIFAR-10 Batch 2:  Loss:     0.5293 Validation Accuracy: 0.635000\n",
      "Epoch 268, CIFAR-10 Batch 3:  Loss:     0.5066 Validation Accuracy: 0.652600\n",
      "Epoch 268, CIFAR-10 Batch 4:  Loss:     0.4909 Validation Accuracy: 0.656000\n",
      "Epoch 268, CIFAR-10 Batch 5:  Loss:     0.4518 Validation Accuracy: 0.643000\n",
      "Epoch 269, CIFAR-10 Batch 1:  Loss:     0.5050 Validation Accuracy: 0.657000\n",
      "Epoch 269, CIFAR-10 Batch 2:  Loss:     0.5250 Validation Accuracy: 0.635200\n",
      "Epoch 269, CIFAR-10 Batch 3:  Loss:     0.5007 Validation Accuracy: 0.652000\n",
      "Epoch 269, CIFAR-10 Batch 4:  Loss:     0.4916 Validation Accuracy: 0.656200\n",
      "Epoch 269, CIFAR-10 Batch 5:  Loss:     0.4495 Validation Accuracy: 0.642400\n",
      "Epoch 270, CIFAR-10 Batch 1:  Loss:     0.5037 Validation Accuracy: 0.656000\n",
      "Epoch 270, CIFAR-10 Batch 2:  Loss:     0.5233 Validation Accuracy: 0.634800\n",
      "Epoch 270, CIFAR-10 Batch 3:  Loss:     0.4996 Validation Accuracy: 0.652600\n",
      "Epoch 270, CIFAR-10 Batch 4:  Loss:     0.4897 Validation Accuracy: 0.656200\n",
      "Epoch 270, CIFAR-10 Batch 5:  Loss:     0.4466 Validation Accuracy: 0.643600\n",
      "Epoch 271, CIFAR-10 Batch 1:  Loss:     0.5006 Validation Accuracy: 0.655800\n",
      "Epoch 271, CIFAR-10 Batch 2:  Loss:     0.5207 Validation Accuracy: 0.634600\n",
      "Epoch 271, CIFAR-10 Batch 3:  Loss:     0.4979 Validation Accuracy: 0.652200\n",
      "Epoch 271, CIFAR-10 Batch 4:  Loss:     0.4894 Validation Accuracy: 0.656000\n",
      "Epoch 271, CIFAR-10 Batch 5:  Loss:     0.4429 Validation Accuracy: 0.645400\n",
      "Epoch 272, CIFAR-10 Batch 1:  Loss:     0.4976 Validation Accuracy: 0.655000\n",
      "Epoch 272, CIFAR-10 Batch 2:  Loss:     0.5170 Validation Accuracy: 0.635000\n",
      "Epoch 272, CIFAR-10 Batch 3:  Loss:     0.4949 Validation Accuracy: 0.652400\n",
      "Epoch 272, CIFAR-10 Batch 4:  Loss:     0.4886 Validation Accuracy: 0.654600\n",
      "Epoch 272, CIFAR-10 Batch 5:  Loss:     0.4407 Validation Accuracy: 0.644800\n",
      "Epoch 273, CIFAR-10 Batch 1:  Loss:     0.4937 Validation Accuracy: 0.655600\n",
      "Epoch 273, CIFAR-10 Batch 2:  Loss:     0.5125 Validation Accuracy: 0.634400\n",
      "Epoch 273, CIFAR-10 Batch 3:  Loss:     0.4928 Validation Accuracy: 0.652000\n",
      "Epoch 273, CIFAR-10 Batch 4:  Loss:     0.4878 Validation Accuracy: 0.654200\n",
      "Epoch 273, CIFAR-10 Batch 5:  Loss:     0.4382 Validation Accuracy: 0.644600\n",
      "Epoch 274, CIFAR-10 Batch 1:  Loss:     0.4920 Validation Accuracy: 0.655000\n",
      "Epoch 274, CIFAR-10 Batch 2:  Loss:     0.5068 Validation Accuracy: 0.635400\n",
      "Epoch 274, CIFAR-10 Batch 3:  Loss:     0.4890 Validation Accuracy: 0.652600\n",
      "Epoch 274, CIFAR-10 Batch 4:  Loss:     0.4870 Validation Accuracy: 0.653000\n",
      "Epoch 274, CIFAR-10 Batch 5:  Loss:     0.4362 Validation Accuracy: 0.643600\n",
      "Epoch 275, CIFAR-10 Batch 1:  Loss:     0.4887 Validation Accuracy: 0.654000\n",
      "Epoch 275, CIFAR-10 Batch 2:  Loss:     0.5021 Validation Accuracy: 0.635400\n",
      "Epoch 275, CIFAR-10 Batch 3:  Loss:     0.4872 Validation Accuracy: 0.652600\n",
      "Epoch 275, CIFAR-10 Batch 4:  Loss:     0.4841 Validation Accuracy: 0.654800\n",
      "Epoch 275, CIFAR-10 Batch 5:  Loss:     0.4329 Validation Accuracy: 0.644600\n",
      "Epoch 276, CIFAR-10 Batch 1:  Loss:     0.4856 Validation Accuracy: 0.654800\n",
      "Epoch 276, CIFAR-10 Batch 2:  Loss:     0.4974 Validation Accuracy: 0.637200\n",
      "Epoch 276, CIFAR-10 Batch 3:  Loss:     0.4850 Validation Accuracy: 0.651800\n",
      "Epoch 276, CIFAR-10 Batch 4:  Loss:     0.4838 Validation Accuracy: 0.654800\n",
      "Epoch 276, CIFAR-10 Batch 5:  Loss:     0.4327 Validation Accuracy: 0.645400\n",
      "Epoch 277, CIFAR-10 Batch 1:  Loss:     0.4837 Validation Accuracy: 0.654200\n",
      "Epoch 277, CIFAR-10 Batch 2:  Loss:     0.4943 Validation Accuracy: 0.637800\n",
      "Epoch 277, CIFAR-10 Batch 3:  Loss:     0.4827 Validation Accuracy: 0.651200\n",
      "Epoch 277, CIFAR-10 Batch 4:  Loss:     0.4837 Validation Accuracy: 0.652800\n",
      "Epoch 277, CIFAR-10 Batch 5:  Loss:     0.4307 Validation Accuracy: 0.645400\n",
      "Epoch 278, CIFAR-10 Batch 1:  Loss:     0.4820 Validation Accuracy: 0.655000\n",
      "Epoch 278, CIFAR-10 Batch 2:  Loss:     0.4903 Validation Accuracy: 0.639600\n",
      "Epoch 278, CIFAR-10 Batch 3:  Loss:     0.4820 Validation Accuracy: 0.651400\n",
      "Epoch 278, CIFAR-10 Batch 4:  Loss:     0.4819 Validation Accuracy: 0.653000\n",
      "Epoch 278, CIFAR-10 Batch 5:  Loss:     0.4272 Validation Accuracy: 0.646200\n",
      "Epoch 279, CIFAR-10 Batch 1:  Loss:     0.4785 Validation Accuracy: 0.655200\n",
      "Epoch 279, CIFAR-10 Batch 2:  Loss:     0.4866 Validation Accuracy: 0.639200\n",
      "Epoch 279, CIFAR-10 Batch 3:  Loss:     0.4806 Validation Accuracy: 0.650600\n",
      "Epoch 279, CIFAR-10 Batch 4:  Loss:     0.4818 Validation Accuracy: 0.651000\n",
      "Epoch 279, CIFAR-10 Batch 5:  Loss:     0.4256 Validation Accuracy: 0.644400\n",
      "Epoch 280, CIFAR-10 Batch 1:  Loss:     0.4765 Validation Accuracy: 0.656200\n",
      "Epoch 280, CIFAR-10 Batch 2:  Loss:     0.4831 Validation Accuracy: 0.641600\n",
      "Epoch 280, CIFAR-10 Batch 3:  Loss:     0.4799 Validation Accuracy: 0.651800\n",
      "Epoch 280, CIFAR-10 Batch 4:  Loss:     0.4823 Validation Accuracy: 0.648000\n",
      "Epoch 280, CIFAR-10 Batch 5:  Loss:     0.4229 Validation Accuracy: 0.644600\n",
      "Epoch 281, CIFAR-10 Batch 1:  Loss:     0.4739 Validation Accuracy: 0.656000\n",
      "Epoch 281, CIFAR-10 Batch 2:  Loss:     0.4791 Validation Accuracy: 0.641400\n",
      "Epoch 281, CIFAR-10 Batch 3:  Loss:     0.4782 Validation Accuracy: 0.652600\n",
      "Epoch 281, CIFAR-10 Batch 4:  Loss:     0.4801 Validation Accuracy: 0.648800\n",
      "Epoch 281, CIFAR-10 Batch 5:  Loss:     0.4208 Validation Accuracy: 0.643400\n",
      "Epoch 282, CIFAR-10 Batch 1:  Loss:     0.4717 Validation Accuracy: 0.656800\n",
      "Epoch 282, CIFAR-10 Batch 2:  Loss:     0.4761 Validation Accuracy: 0.642600\n",
      "Epoch 282, CIFAR-10 Batch 3:  Loss:     0.4773 Validation Accuracy: 0.652200\n",
      "Epoch 282, CIFAR-10 Batch 4:  Loss:     0.4779 Validation Accuracy: 0.648800\n",
      "Epoch 282, CIFAR-10 Batch 5:  Loss:     0.4171 Validation Accuracy: 0.644400\n",
      "Epoch 283, CIFAR-10 Batch 1:  Loss:     0.4688 Validation Accuracy: 0.656000\n",
      "Epoch 283, CIFAR-10 Batch 2:  Loss:     0.4727 Validation Accuracy: 0.642800\n",
      "Epoch 283, CIFAR-10 Batch 3:  Loss:     0.4764 Validation Accuracy: 0.651600\n",
      "Epoch 283, CIFAR-10 Batch 4:  Loss:     0.4767 Validation Accuracy: 0.648400\n",
      "Epoch 283, CIFAR-10 Batch 5:  Loss:     0.4129 Validation Accuracy: 0.646400\n",
      "Epoch 284, CIFAR-10 Batch 1:  Loss:     0.4669 Validation Accuracy: 0.655000\n",
      "Epoch 284, CIFAR-10 Batch 2:  Loss:     0.4706 Validation Accuracy: 0.642600\n",
      "Epoch 284, CIFAR-10 Batch 3:  Loss:     0.4753 Validation Accuracy: 0.651800\n",
      "Epoch 284, CIFAR-10 Batch 4:  Loss:     0.4750 Validation Accuracy: 0.647400\n",
      "Epoch 284, CIFAR-10 Batch 5:  Loss:     0.4108 Validation Accuracy: 0.648400\n",
      "Epoch 285, CIFAR-10 Batch 1:  Loss:     0.4664 Validation Accuracy: 0.654400\n",
      "Epoch 285, CIFAR-10 Batch 2:  Loss:     0.4679 Validation Accuracy: 0.641800\n",
      "Epoch 285, CIFAR-10 Batch 3:  Loss:     0.4740 Validation Accuracy: 0.653000\n",
      "Epoch 285, CIFAR-10 Batch 4:  Loss:     0.4727 Validation Accuracy: 0.646400\n",
      "Epoch 285, CIFAR-10 Batch 5:  Loss:     0.4066 Validation Accuracy: 0.649400\n",
      "Epoch 286, CIFAR-10 Batch 1:  Loss:     0.4637 Validation Accuracy: 0.655600\n",
      "Epoch 286, CIFAR-10 Batch 2:  Loss:     0.4665 Validation Accuracy: 0.641800\n",
      "Epoch 286, CIFAR-10 Batch 3:  Loss:     0.4725 Validation Accuracy: 0.652200\n",
      "Epoch 286, CIFAR-10 Batch 4:  Loss:     0.4692 Validation Accuracy: 0.648200\n",
      "Epoch 286, CIFAR-10 Batch 5:  Loss:     0.4031 Validation Accuracy: 0.650200\n",
      "Epoch 287, CIFAR-10 Batch 1:  Loss:     0.4624 Validation Accuracy: 0.655200\n",
      "Epoch 287, CIFAR-10 Batch 2:  Loss:     0.4648 Validation Accuracy: 0.641800\n",
      "Epoch 287, CIFAR-10 Batch 3:  Loss:     0.4733 Validation Accuracy: 0.652600\n",
      "Epoch 287, CIFAR-10 Batch 4:  Loss:     0.4654 Validation Accuracy: 0.646200\n",
      "Epoch 287, CIFAR-10 Batch 5:  Loss:     0.4006 Validation Accuracy: 0.649800\n",
      "Epoch 288, CIFAR-10 Batch 1:  Loss:     0.4622 Validation Accuracy: 0.655800\n",
      "Epoch 288, CIFAR-10 Batch 2:  Loss:     0.4626 Validation Accuracy: 0.643200\n",
      "Epoch 288, CIFAR-10 Batch 3:  Loss:     0.4740 Validation Accuracy: 0.652800\n",
      "Epoch 288, CIFAR-10 Batch 4:  Loss:     0.4634 Validation Accuracy: 0.647800\n",
      "Epoch 288, CIFAR-10 Batch 5:  Loss:     0.3991 Validation Accuracy: 0.649400\n",
      "Epoch 289, CIFAR-10 Batch 1:  Loss:     0.4621 Validation Accuracy: 0.653800\n",
      "Epoch 289, CIFAR-10 Batch 2:  Loss:     0.4597 Validation Accuracy: 0.644000\n",
      "Epoch 289, CIFAR-10 Batch 3:  Loss:     0.4734 Validation Accuracy: 0.651400\n",
      "Epoch 289, CIFAR-10 Batch 4:  Loss:     0.4613 Validation Accuracy: 0.648200\n",
      "Epoch 289, CIFAR-10 Batch 5:  Loss:     0.3986 Validation Accuracy: 0.650000\n",
      "Epoch 290, CIFAR-10 Batch 1:  Loss:     0.4603 Validation Accuracy: 0.652400\n",
      "Epoch 290, CIFAR-10 Batch 2:  Loss:     0.4579 Validation Accuracy: 0.643400\n",
      "Epoch 290, CIFAR-10 Batch 3:  Loss:     0.4740 Validation Accuracy: 0.650600\n",
      "Epoch 290, CIFAR-10 Batch 4:  Loss:     0.4589 Validation Accuracy: 0.645800\n",
      "Epoch 290, CIFAR-10 Batch 5:  Loss:     0.3975 Validation Accuracy: 0.648400\n",
      "Epoch 291, CIFAR-10 Batch 1:  Loss:     0.4606 Validation Accuracy: 0.654000\n",
      "Epoch 291, CIFAR-10 Batch 2:  Loss:     0.4554 Validation Accuracy: 0.643000\n",
      "Epoch 291, CIFAR-10 Batch 3:  Loss:     0.4720 Validation Accuracy: 0.649600\n",
      "Epoch 291, CIFAR-10 Batch 4:  Loss:     0.4574 Validation Accuracy: 0.646400\n",
      "Epoch 291, CIFAR-10 Batch 5:  Loss:     0.3973 Validation Accuracy: 0.648200\n",
      "Epoch 292, CIFAR-10 Batch 1:  Loss:     0.4612 Validation Accuracy: 0.653000\n",
      "Epoch 292, CIFAR-10 Batch 2:  Loss:     0.4535 Validation Accuracy: 0.642400\n",
      "Epoch 292, CIFAR-10 Batch 3:  Loss:     0.4713 Validation Accuracy: 0.649800\n",
      "Epoch 292, CIFAR-10 Batch 4:  Loss:     0.4565 Validation Accuracy: 0.647000\n",
      "Epoch 292, CIFAR-10 Batch 5:  Loss:     0.3965 Validation Accuracy: 0.647600\n",
      "Epoch 293, CIFAR-10 Batch 1:  Loss:     0.4595 Validation Accuracy: 0.653000\n",
      "Epoch 293, CIFAR-10 Batch 2:  Loss:     0.4511 Validation Accuracy: 0.643200\n",
      "Epoch 293, CIFAR-10 Batch 3:  Loss:     0.4709 Validation Accuracy: 0.648800\n",
      "Epoch 293, CIFAR-10 Batch 4:  Loss:     0.4538 Validation Accuracy: 0.647800\n",
      "Epoch 293, CIFAR-10 Batch 5:  Loss:     0.3961 Validation Accuracy: 0.648400\n",
      "Epoch 294, CIFAR-10 Batch 1:  Loss:     0.4598 Validation Accuracy: 0.653600\n",
      "Epoch 294, CIFAR-10 Batch 2:  Loss:     0.4499 Validation Accuracy: 0.643200\n",
      "Epoch 294, CIFAR-10 Batch 3:  Loss:     0.4697 Validation Accuracy: 0.648600\n",
      "Epoch 294, CIFAR-10 Batch 4:  Loss:     0.4536 Validation Accuracy: 0.645800\n",
      "Epoch 294, CIFAR-10 Batch 5:  Loss:     0.3946 Validation Accuracy: 0.647600\n",
      "Epoch 295, CIFAR-10 Batch 1:  Loss:     0.4590 Validation Accuracy: 0.653400\n",
      "Epoch 295, CIFAR-10 Batch 2:  Loss:     0.4484 Validation Accuracy: 0.643400\n",
      "Epoch 295, CIFAR-10 Batch 3:  Loss:     0.4693 Validation Accuracy: 0.647400\n",
      "Epoch 295, CIFAR-10 Batch 4:  Loss:     0.4544 Validation Accuracy: 0.644600\n",
      "Epoch 295, CIFAR-10 Batch 5:  Loss:     0.3949 Validation Accuracy: 0.647400\n",
      "Epoch 296, CIFAR-10 Batch 1:  Loss:     0.4574 Validation Accuracy: 0.653000\n",
      "Epoch 296, CIFAR-10 Batch 2:  Loss:     0.4469 Validation Accuracy: 0.642800\n",
      "Epoch 296, CIFAR-10 Batch 3:  Loss:     0.4688 Validation Accuracy: 0.647000\n",
      "Epoch 296, CIFAR-10 Batch 4:  Loss:     0.4538 Validation Accuracy: 0.644200\n",
      "Epoch 296, CIFAR-10 Batch 5:  Loss:     0.3956 Validation Accuracy: 0.647600\n",
      "Epoch 297, CIFAR-10 Batch 1:  Loss:     0.4563 Validation Accuracy: 0.653200\n",
      "Epoch 297, CIFAR-10 Batch 2:  Loss:     0.4464 Validation Accuracy: 0.644600\n",
      "Epoch 297, CIFAR-10 Batch 3:  Loss:     0.4670 Validation Accuracy: 0.647200\n",
      "Epoch 297, CIFAR-10 Batch 4:  Loss:     0.4557 Validation Accuracy: 0.641000\n",
      "Epoch 297, CIFAR-10 Batch 5:  Loss:     0.3942 Validation Accuracy: 0.647600\n",
      "Epoch 298, CIFAR-10 Batch 1:  Loss:     0.4554 Validation Accuracy: 0.652800\n",
      "Epoch 298, CIFAR-10 Batch 2:  Loss:     0.4465 Validation Accuracy: 0.642800\n",
      "Epoch 298, CIFAR-10 Batch 3:  Loss:     0.4684 Validation Accuracy: 0.648000\n",
      "Epoch 298, CIFAR-10 Batch 4:  Loss:     0.4599 Validation Accuracy: 0.641200\n",
      "Epoch 298, CIFAR-10 Batch 5:  Loss:     0.3939 Validation Accuracy: 0.647400\n",
      "Epoch 299, CIFAR-10 Batch 1:  Loss:     0.4550 Validation Accuracy: 0.654400\n",
      "Epoch 299, CIFAR-10 Batch 2:  Loss:     0.4488 Validation Accuracy: 0.643400\n",
      "Epoch 299, CIFAR-10 Batch 3:  Loss:     0.4670 Validation Accuracy: 0.648800\n",
      "Epoch 299, CIFAR-10 Batch 4:  Loss:     0.4676 Validation Accuracy: 0.635200\n",
      "Epoch 299, CIFAR-10 Batch 5:  Loss:     0.3945 Validation Accuracy: 0.648000\n",
      "Epoch 300, CIFAR-10 Batch 1:  Loss:     0.4541 Validation Accuracy: 0.653800\n",
      "Epoch 300, CIFAR-10 Batch 2:  Loss:     0.4529 Validation Accuracy: 0.637600\n",
      "Epoch 300, CIFAR-10 Batch 3:  Loss:     0.4646 Validation Accuracy: 0.650400\n",
      "Epoch 300, CIFAR-10 Batch 4:  Loss:     0.4804 Validation Accuracy: 0.632200\n",
      "Epoch 300, CIFAR-10 Batch 5:  Loss:     0.3929 Validation Accuracy: 0.646400\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6475705504417419\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ea82b8a4a8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
